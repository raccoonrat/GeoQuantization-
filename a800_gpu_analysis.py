#!/usr/bin/env python3
"""
A800 GPUå†…å­˜åˆ†æå·¥å…·
åˆ†æåœ¨80GB GPUä¸Šè¿›è¡ŒHessianè®¡ç®—çš„å¯è¡Œæ€§
"""

import os
import sys
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜"""
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        logger.info(f"å¯ç”¨GPUæ•°é‡: {gpu_count}")
        
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            total_memory = props.total_memory / (1024**3)  # GB
            logger.info(f"GPU {i}: {props.name}")
            logger.info(f"  æ€»å†…å­˜: {total_memory:.1f} GB")
            logger.info(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        
        return True
    else:
        logger.warning("CUDAä¸å¯ç”¨")
        return False

def analyze_model_memory_on_gpu(model_name):
    """åˆ†ææ¨¡å‹åœ¨GPUä¸Šçš„å†…å­˜ä½¿ç”¨"""
    logger.info(f"åˆ†ææ¨¡å‹: {model_name}")
    
    try:
        # æ£€æŸ¥GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated() / (1024**3)
            logger.info(f"åˆå§‹GPUå†…å­˜ä½¿ç”¨: {initial_memory:.2f} GB")
        
        # åŠ è½½æ¨¡å‹åˆ°GPU
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,  # ä½¿ç”¨float16èŠ‚çœå†…å­˜
            device_map="auto"
        )
        
        if torch.cuda.is_available():
            model_memory = torch.cuda.memory_allocated() / (1024**3)
            logger.info(f"æ¨¡å‹åŠ è½½åGPUå†…å­˜: {model_memory:.2f} GB")
            logger.info(f"æ¨¡å‹å ç”¨å†…å­˜: {model_memory - initial_memory:.2f} GB")
        
        # è·å–å‚æ•°ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        logger.info(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
        logger.info(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
        
        # ä¼°ç®—Hessianå†…å­˜éœ€æ±‚
        hessian_memory_estimates = {
            'diagonal_hessian_fp16': trainable_params * 2 / (1024**3),  # float16
            'diagonal_hessian_fp32': trainable_params * 4 / (1024**3),  # float32
            'block_hessian_fp16': trainable_params * 10 * 2 / (1024**3),  # 10x block
            'full_hessian_fp16': trainable_params * trainable_params * 2 / (1024**3),
        }
        
        logger.info("Hessianå†…å­˜éœ€æ±‚ä¼°ç®—:")
        for method, memory_gb in hessian_memory_estimates.items():
            logger.info(f"  {method}: {memory_gb:.2f} GB")
        
        # æ¸…ç†å†…å­˜
        del model
        torch.cuda.empty_cache()
        
        return {
            'model_name': model_name,
            'total_params': total_params,
            'trainable_params': trainable_params,
            'model_memory_gb': model_memory - initial_memory if torch.cuda.is_available() else 0,
            'hessian_memory_estimates': hessian_memory_estimates
        }
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åˆ†æå¤±è´¥: {e}")
        return None

def analyze_a800_capabilities():
    """åˆ†æA800çš„èƒ½åŠ›"""
    logger.info("A800 GPUèƒ½åŠ›åˆ†æ")
    logger.info("=" * 50)
    
    # A800è§„æ ¼
    a800_specs = {
        'total_memory': 80,  # GB
        'memory_bandwidth': 2039,  # GB/s
        'compute_capability': '8.0',
        'cuda_cores': 6912,
        'tensor_cores': 432,
        'fp16_performance': 312,  # TFLOPS
        'fp32_performance': 156,  # TFLOPS
    }
    
    logger.info("A800è§„æ ¼:")
    for key, value in a800_specs.items():
        logger.info(f"  {key}: {value}")
    
    # å¯ç”¨å†…å­˜ä¼°ç®—ï¼ˆè€ƒè™‘ç³»ç»Ÿå¼€é”€ï¼‰
    usable_memory = a800_specs['total_memory'] * 0.9  # 90%å¯ç”¨
    logger.info(f"ä¼°ç®—å¯ç”¨å†…å­˜: {usable_memory:.1f} GB")
    
    return a800_specs, usable_memory

def recommend_models_for_a800():
    """æ¨èé€‚åˆA800çš„æ¨¡å‹"""
    logger.info("æ¨èæ¨¡å‹é…ç½®")
    logger.info("=" * 50)
    
    models = [
        {
            'name': 'facebook/opt-125m',
            'params': 125_000_000,
            'estimated_memory': 0.5,
            'hessian_diagonal_fp16': 0.25,
            'recommended': True,
            'notes': 'é€‚åˆå®Œæ•´Hessianè®¡ç®—'
        },
        {
            'name': 'facebook/opt-350m',
            'params': 350_000_000,
            'estimated_memory': 1.4,
            'hessian_diagonal_fp16': 0.7,
            'recommended': True,
            'notes': 'é€‚åˆå®Œæ•´Hessianè®¡ç®—'
        },
        {
            'name': 'facebook/opt-1.3b',
            'params': 1_300_000_000,
            'estimated_memory': 5.2,
            'hessian_diagonal_fp16': 2.6,
            'recommended': True,
            'notes': 'é€‚åˆå®Œæ•´Hessianè®¡ç®—'
        },
        {
            'name': 'facebook/opt-2.7b',
            'params': 2_700_000_000,
            'estimated_memory': 10.8,
            'hessian_diagonal_fp16': 5.4,
            'recommended': True,
            'notes': 'é€‚åˆå®Œæ•´Hessianè®¡ç®—'
        },
        {
            'name': 'facebook/opt-6.7b',
            'params': 6_700_000_000,
            'estimated_memory': 26.8,
            'hessian_diagonal_fp16': 13.4,
            'recommended': True,
            'notes': 'é€‚åˆå®Œæ•´Hessianè®¡ç®—'
        },
        {
            'name': 'facebook/opt-13b',
            'params': 13_000_000_000,
            'estimated_memory': 52.0,
            'hessian_diagonal_fp16': 26.0,
            'recommended': True,
            'notes': 'é€‚åˆå®Œæ•´Hessianè®¡ç®—'
        },
        {
            'name': 'facebook/opt-30b',
            'params': 30_000_000_000,
            'estimated_memory': 120.0,
            'hessian_diagonal_fp16': 60.0,
            'recommended': False,
            'notes': 'éœ€è¦æ¨¡å‹å¹¶è¡Œæˆ–é‡åŒ–'
        }
    ]
    
    for model in models:
        status = "âœ… æ¨è" if model['recommended'] else "âŒ ä¸æ¨è"
        logger.info(f"{model['name']}: {status}")
        logger.info(f"  å‚æ•°æ•°é‡: {model['params']:,}")
        logger.info(f"  æ¨¡å‹å†…å­˜: {model['estimated_memory']:.1f} GB")
        logger.info(f"  å¯¹è§’Hessian: {model['hessian_diagonal_fp16']:.1f} GB")
        logger.info(f"  è¯´æ˜: {model['notes']}")
        logger.info("")

def create_a800_optimization_plan():
    """åˆ›å»ºA800ä¼˜åŒ–æ–¹æ¡ˆ"""
    logger.info("A800ä¼˜åŒ–æ–¹æ¡ˆ")
    logger.info("=" * 50)
    
    optimizations = [
        {
            'name': 'æ··åˆç²¾åº¦è®­ç»ƒ',
            'description': 'ä½¿ç”¨float16å‡å°‘å†…å­˜ä½¿ç”¨',
            'memory_saving': '50%',
            'implementation': 'torch.cuda.amp.autocast()',
            'recommended': True
        },
        {
            'name': 'æ¢¯åº¦ç´¯ç§¯',
            'description': 'ç´¯ç§¯æ¢¯åº¦å‡å°‘å†…å­˜å³°å€¼',
            'memory_saving': '30-50%',
            'implementation': 'accumulation_steps=4',
            'recommended': True
        },
        {
            'name': 'æ¨¡å‹å¹¶è¡Œ',
            'description': 'å°†æ¨¡å‹åˆ†ç‰‡åˆ°å¤šä¸ªGPU',
            'memory_saving': 'æŒ‰GPUæ•°é‡çº¿æ€§å‡å°‘',
            'implementation': 'torch.nn.parallel.DistributedDataParallel',
            'recommended': False
        },
        {
            'name': 'æ¿€æ´»æ£€æŸ¥ç‚¹',
            'description': 'é‡æ–°è®¡ç®—æ¿€æ´»èŠ‚çœå†…å­˜',
            'memory_saving': '50-70%',
            'implementation': 'model.gradient_checkpointing_enable()',
            'recommended': True
        },
        {
            'name': 'Hessianåˆ†å—è®¡ç®—',
            'description': 'åˆ†å—è®¡ç®—HessiançŸ©é˜µ',
            'memory_saving': '80%+',
            'implementation': 'chunk_size=1000',
            'recommended': True
        },
        {
            'name': 'åŠ¨æ€æ‰¹å¤„ç†',
            'description': 'æ ¹æ®å†…å­˜åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°',
            'memory_saving': 'è‡ªé€‚åº”',
            'implementation': 'adaptive_batch_size',
            'recommended': True
        }
    ]
    
    for opt in optimizations:
        status = "âœ… æ¨è" if opt['recommended'] else "âš ï¸ å¯é€‰"
        logger.info(f"{opt['name']}: {status}")
        logger.info(f"  æè¿°: {opt['description']}")
        logger.info(f"  å†…å­˜èŠ‚çœ: {opt['memory_saving']}")
        logger.info(f"  å®ç°: {opt['implementation']}")
        logger.info("")

def estimate_experiment_memory():
    """ä¼°ç®—å®Œæ•´å®éªŒçš„å†…å­˜éœ€æ±‚"""
    logger.info("å®Œæ•´å®éªŒå†…å­˜éœ€æ±‚ä¼°ç®—")
    logger.info("=" * 50)
    
    # ä»¥OPT-6.7Bä¸ºä¾‹
    model_memory = 26.8  # GB
    hessian_diagonal = 13.4  # GB
    activations = 2.0  # GB
    gradients = 13.4  # GB
    intermediate = 5.0  # GB
    
    total_memory = model_memory + hessian_diagonal + activations + gradients + intermediate
    
    logger.info(f"æ¨¡å‹å†…å­˜: {model_memory:.1f} GB")
    logger.info(f"Hessianå¯¹è§’: {hessian_diagonal:.1f} GB")
    logger.info(f"æ¿€æ´»å†…å­˜: {activations:.1f} GB")
    logger.info(f"æ¢¯åº¦å†…å­˜: {gradients:.1f} GB")
    logger.info(f"ä¸­é—´å˜é‡: {intermediate:.1f} GB")
    logger.info(f"æ€»å†…å­˜éœ€æ±‚: {total_memory:.1f} GB")
    
    a800_memory = 80
    memory_usage = (total_memory / a800_memory) * 100
    
    logger.info(f"A800æ€»å†…å­˜: {a800_memory} GB")
    logger.info(f"å†…å­˜ä½¿ç”¨ç‡: {memory_usage:.1f}%")
    
    if memory_usage < 80:
        logger.info("âœ… å†…å­˜å……è¶³ï¼Œå¯ä»¥è¿è¡Œå®Œæ•´å®éªŒ")
    elif memory_usage < 95:
        logger.info("âš ï¸ å†…å­˜ç´§å¼ ï¼Œå»ºè®®ä½¿ç”¨ä¼˜åŒ–æŠ€æœ¯")
    else:
        logger.info("âŒ å†…å­˜ä¸è¶³ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ A800 GPUå†…å­˜åˆ†æå·¥å…·")
    logger.info("=" * 60)
    
    # æ£€æŸ¥GPU
    if not check_gpu_memory():
        logger.error("GPUä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        return
    
    # åˆ†æA800èƒ½åŠ›
    a800_specs, usable_memory = analyze_a800_capabilities()
    
    # æ¨èæ¨¡å‹
    recommend_models_for_a800()
    
    # ä¼˜åŒ–æ–¹æ¡ˆ
    create_a800_optimization_plan()
    
    # å†…å­˜éœ€æ±‚ä¼°ç®—
    estimate_experiment_memory()
    
    logger.info("=" * 60)
    logger.info("âœ… A800åˆ†æå®Œæˆ")
    logger.info("ç»“è®º: A800 80GB GPUå®Œå…¨è¶³å¤Ÿè¿›è¡ŒHessianè®¡ç®—å®éªŒ")

if __name__ == "__main__":
    main()
