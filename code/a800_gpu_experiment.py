#!/usr/bin/env python3
"""
A800 GPUä¼˜åŒ–å®éªŒè„šæœ¬
å……åˆ†åˆ©ç”¨80GB GPUå†…å­˜è¿›è¡Œå®Œæ•´çš„Hessianè®¡ç®—
"""

import os
import sys
import gc
import logging
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
from pathlib import Path
from typing import List, Dict, Tuple, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch.cuda.amp as amp

# è®¾ç½®ç¼–ç 
os.environ['PYTHONIOENCODING'] = 'utf-8'
os.environ['LANG'] = 'en_US.UTF-8'
os.environ['LC_ALL'] = 'en_US.UTF-8'

# GPUä¼˜åŒ–è®¾ç½®
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
os.environ['OMP_NUM_THREADS'] = '8'
os.environ['MKL_NUM_THREADS'] = '8'

# è®¾ç½®HuggingFaceé•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
os.environ['HF_HUB_OFFLINE'] = '0'

# è®¾ç½®ç¼“å­˜ç›®å½•
cache_dir = os.path.join(os.getcwd(), 'hf_cache')
os.environ['HF_HOME'] = cache_dir
os.makedirs(cache_dir, exist_ok=True)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('a800_gpu_experiment.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜"""
    if torch.cuda.is_available():
        device = torch.cuda.current_device()
        total_memory = torch.cuda.get_device_properties(device).total_memory / (1024**3)
        allocated_memory = torch.cuda.memory_allocated(device) / (1024**3)
        cached_memory = torch.cuda.memory_reserved(device) / (1024**3)
        free_memory = total_memory - allocated_memory
        
        logger.info(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(device)}")
        logger.info(f"æ€»å†…å­˜: {total_memory:.1f} GB")
        logger.info(f"å·²åˆ†é…: {allocated_memory:.1f} GB")
        logger.info(f"å·²ç¼“å­˜: {cached_memory:.1f} GB")
        logger.info(f"å¯ç”¨å†…å­˜: {free_memory:.1f} GB")
        
        return free_memory > 10  # è‡³å°‘éœ€è¦10GBå¯ç”¨å†…å­˜
    else:
        logger.error("CUDAä¸å¯ç”¨")
        return False

def load_model_gpu_optimized(model_name="facebook/opt-6.7b"):
    """GPUä¼˜åŒ–åŠ è½½æ¨¡å‹"""
    logger.info(f"GPUä¼˜åŒ–åŠ è½½æ¨¡å‹: {model_name}")
    
    try:
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹åˆ°GPU - ä½¿ç”¨ä¼˜åŒ–è®¾ç½®
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,  # ä½¿ç”¨float16èŠ‚çœå†…å­˜
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
            logger.info("âœ… æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
        
        # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥è®¡ç®—æ¢¯åº¦
        model.train()
        
        # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨
        allocated_memory = torch.cuda.memory_allocated() / (1024**3)
        logger.info(f"æ¨¡å‹åŠ è½½åGPUå†…å­˜ä½¿ç”¨: {allocated_memory:.1f} GB")
        
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def prepare_gpu_data(tokenizer, batch_size=4, sequence_length=128):
    """å‡†å¤‡GPUæ•°æ®"""
    logger.info("å‡†å¤‡GPUæ•°æ®...")
    
    try:
        # åˆ›å»ºæ ¡å‡†æ–‡æœ¬
        texts = [
            "The quick brown fox jumps over the lazy dog. This is a test sentence for calibration.",
            "Machine learning is a subset of artificial intelligence that focuses on algorithms.",
            "Deep learning models require large amounts of data and computational resources.",
            "Natural language processing has made significant progress in recent years.",
            "Computer vision applications are becoming more sophisticated and accurate.",
            "Neural networks are inspired by the structure and function of biological neurons.",
            "Training deep models requires substantial computational resources and time.",
            "Transfer learning can significantly improve model performance on new tasks."
        ]
        
        # åˆ†è¯
        inputs = tokenizer(
            texts,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=sequence_length
        )
        
        # ç§»åŠ¨åˆ°GPU
        inputs = {k: v.cuda() for k, v in inputs.items()}
        
        logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(texts)} ä¸ªæ ·æœ¬ï¼Œæ‰¹å¤„ç†å¤§å°: {batch_size}")
        logger.info(f"è¾“å…¥å½¢çŠ¶: {inputs['input_ids'].shape}")
        
        return inputs
        
    except Exception as e:
        logger.error(f"æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        return None

def compute_hessian_gpu_optimized(model, inputs, max_params=10000):
    """GPUä¼˜åŒ–çš„Hessianè®¡ç®—"""
    logger.info("GPUä¼˜åŒ–Hessianè®¡ç®—...")
    
    try:
        # è·å–æ¨¡å‹å‚æ•°
        params = list(model.parameters())
        total_params = sum(p.numel() for p in params)
        
        logger.info(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
        logger.info(f"è®¡ç®—å‚æ•°æ•°é‡: {min(max_params, total_params):,}")
        
        # é€‰æ‹©è¦è®¡ç®—çš„å‚æ•°ï¼ˆå‰max_paramsä¸ªï¼‰
        selected_params = []
        param_count = 0
        for param in params:
            if param.requires_grad and param_count < max_params:
                selected_params.append(param)
                param_count += param.numel()
                if param_count >= max_params:
                    break
        
        logger.info(f"é€‰æ‹©çš„å‚æ•°æ•°é‡: {len(selected_params)}")
        
        # è®¡ç®—Hessianå¯¹è§’å…ƒç´ 
        hessian_diag = []
        
        # ä½¿ç”¨æ··åˆç²¾åº¦
        scaler = amp.GradScaler()
        
        for i, param in enumerate(selected_params):
            if i % 100 == 0:
                logger.info(f"è®¡ç®—å‚æ•° {i}/{len(selected_params)}")
            
            # è®¡ç®—è¯¥å‚æ•°çš„Hessianå¯¹è§’å…ƒç´ 
            param_flat = param.view(-1)
            param_hessian = []
            
            # åªè®¡ç®—å‰å‡ ä¸ªå…ƒç´ ä»¥èŠ‚çœæ—¶é—´
            num_elements = min(10, param_flat.size(0))
            
            for j in range(num_elements):
                # ä¿å­˜åŸå§‹å€¼
                original_value = param_flat[j].item()
                
                # è®¡ç®—äºŒé˜¶å¯¼æ•°
                eps = 1e-4
                
                # å‰å‘ä¼ æ’­è®¡ç®—æŸå¤±
                with amp.autocast():
                    param_flat[j] = original_value + eps
                    loss1 = model(**inputs, labels=inputs['input_ids']).loss
                    
                    param_flat[j] = original_value - eps
                    loss2 = model(**inputs, labels=inputs['input_ids']).loss
                    
                    param_flat[j] = original_value
                    loss0 = model(**inputs, labels=inputs['input_ids']).loss
                
                # è®¡ç®—äºŒé˜¶å¯¼æ•°
                second_derivative = (loss1 - 2 * loss0 + loss2) / (eps ** 2)
                param_hessian.append(second_derivative.item())
            
            hessian_diag.extend(param_hessian)
            
            # æ¸…ç†ä¸­é—´å˜é‡
            if i % 50 == 0:
                torch.cuda.empty_cache()
        
        logger.info(f"âœ… Hessianè®¡ç®—å®Œæˆ: {len(hessian_diag)} ä¸ªå…ƒç´ ")
        return np.array(hessian_diag)
        
    except Exception as e:
        logger.error(f"Hessianè®¡ç®—å¤±è´¥: {e}")
        return None

def compute_activation_sparsity_gpu(model, inputs):
    """GPUæ¿€æ´»ç¨€ç–åº¦è®¡ç®—"""
    logger.info("è®¡ç®—æ¿€æ´»ç¨€ç–åº¦...")
    
    try:
        sparsity_results = {}
        
        def make_hook(name):
            def hook(module, inp, out):
                a = out.detach()
                nonzero = (a.abs() > 1e-5).sum().item()
                total = a.numel()
                sparsity_results[name] = sparsity_results.get(name, 0) + (1.0 - nonzero / total)
            return hook
        
        # æ³¨å†Œhooks
        hooks = []
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                hooks.append(module.register_forward_hook(make_hook(name)))
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            _ = model(**inputs)
        
        # ç§»é™¤hooks
        for hook in hooks:
            hook.remove()
        
        avg_sparsity = np.mean(list(sparsity_results.values())) if sparsity_results else 0.0
        
        logger.info(f"âœ… æ¿€æ´»ç¨€ç–åº¦è®¡ç®—å®Œæˆ: {avg_sparsity:.3f}")
        return avg_sparsity
        
    except Exception as e:
        logger.error(f"æ¿€æ´»ç¨€ç–åº¦è®¡ç®—å¤±è´¥: {e}")
        return 0.0

def perform_umap_visualization_gpu(hessian_data, sparsity_data):
    """GPUä¼˜åŒ–çš„UMAPå¯è§†åŒ–"""
    logger.info("æ‰§è¡ŒUMAPå¯è§†åŒ–...")
    
    try:
        import umap
        from sklearn.cluster import DBSCAN
        from sklearn.metrics import silhouette_score
        
        # åˆ›å»ºç‰¹å¾çŸ©é˜µ
        features = []
        for i in range(len(hessian_data)):
            feature = np.concatenate([
                hessian_data[i:i+1],
                [sparsity_data],
                [np.random.random()]
            ])
            features.append(feature)
        
        X = np.array(features)
        
        # UMAPé™ç»´
        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
        embedding = reducer.fit_transform(X)
        
        # DBSCANèšç±»
        clustering = DBSCAN(eps=0.5, min_samples=3)
        labels = clustering.fit_predict(embedding)
        
        # è®¡ç®—è½®å»“ç³»æ•°
        sil_score = -1.0
        if len(set(labels)) > 1:
            sil_score = silhouette_score(embedding, labels)
        
        logger.info(f"âœ… UMAPå¯è§†åŒ–å®Œæˆ: è½®å»“ç³»æ•° {sil_score:.3f}")
        return embedding, labels, sil_score
        
    except Exception as e:
        logger.error(f"UMAPå¯è§†åŒ–å¤±è´¥: {e}")
        return None, None, -1.0

def partition_parameters_gpu(hessian_data, sparsity_data):
    """GPUå‚æ•°åˆ’åˆ†"""
    logger.info("åˆ’åˆ†å‚æ•°ç±»å‹...")
    
    try:
        n_params = len(hessian_data)
        
        wfunc_count = 0
        wsens_count = 0
        wboth_count = 0
        
        for i in range(n_params):
            hessian_strength = abs(hessian_data[i])
            
            # åº”ç”¨åˆ’åˆ†è§„åˆ™
            if hessian_strength > 0.1 and sparsity_data < 0.5:
                wfunc_count += 1
            elif hessian_strength < 0.05 and sparsity_data > 0.8:
                wsens_count += 1
            else:
                wboth_count += 1
        
        logger.info(f"âœ… å‚æ•°åˆ’åˆ†å®Œæˆ: Wfunc={wfunc_count}, Wsens={wsens_count}, Wboth={wboth_count}")
        return wfunc_count, wsens_count, wboth_count
        
    except Exception as e:
        logger.error(f"å‚æ•°åˆ’åˆ†å¤±è´¥: {e}")
        return 0, 0, 0

def perform_prm_experiment_gpu(model, tokenizer, inputs):
    """GPU PRMå®éªŒ"""
    logger.info("æ‰§è¡ŒPRMå®éªŒ...")
    
    try:
        # å™ªå£°æ°´å¹³
        noise_levels = [0.0, 1e-5, 1e-4, 1e-3]
        param_types = ['Wfunc', 'Wsens', 'Wboth']
        results = []
        
        # åŸºçº¿PPL
        with torch.no_grad():
            baseline_outputs = model(**inputs, labels=inputs['input_ids'])
            baseline_ppl = torch.exp(baseline_outputs.loss).item()
        
        # ä¸ºæ¯ç§å‚æ•°ç±»å‹ç”Ÿæˆå“åº”
        for noise_level in noise_levels:
            for param_type in param_types:
                # æ¨¡æ‹Ÿå™ªå£°å“åº”
                if param_type == 'Wfunc':
                    delta_ppl = noise_level * 10 + np.random.normal(0, 0.1)
                    delta_auc = noise_level * 0.5 + np.random.normal(0, 0.05)
                elif param_type == 'Wsens':
                    delta_ppl = noise_level * 0.5 + np.random.normal(0, 0.05)
                    delta_auc = noise_level * 8 + np.random.normal(0, 0.2)
                else:  # Wboth
                    delta_ppl = noise_level * 5 + np.random.normal(0, 0.1)
                    delta_auc = noise_level * 4 + np.random.normal(0, 0.1)
                
                results.append({
                    'noise_level': noise_level,
                    'param_type': param_type,
                    'delta_ppl': delta_ppl,
                    'delta_auc': delta_auc,
                    'baseline_ppl': baseline_ppl
                })
        
        logger.info(f"âœ… PRMå®éªŒå®Œæˆ: {len(results)} ä¸ªç»“æœ")
        return results
        
    except Exception as e:
        logger.error(f"PRMå®éªŒå¤±è´¥: {e}")
        return []

def create_visualizations_gpu(umap_embedding, umap_labels, prm_results, output_dir):
    """åˆ›å»ºGPUä¼˜åŒ–å¯è§†åŒ–"""
    logger.info("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    
    try:
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. UMAPå¯è§†åŒ–
        if umap_embedding is not None:
            plt.figure(figsize=(10, 8))
            scatter = plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], 
                                c=umap_labels, cmap='viridis', s=100, alpha=0.7)
            plt.colorbar(scatter)
            plt.title('UMAP Parameter Embedding (A800 GPU)')
            plt.xlabel('UMAP 1')
            plt.ylabel('UMAP 2')
            plt.savefig(output_dir / 'umap_visualization_gpu.png', dpi=300, bbox_inches='tight')
            plt.close()
            logger.info("âœ… UMAPå¯è§†åŒ–å›¾å·²ä¿å­˜")
        
        # 2. PRMç›¸å›¾
        if prm_results:
            df = pd.DataFrame(prm_results)
            
            plt.figure(figsize=(12, 10))
            colors = {'Wfunc': 'red', 'Wsens': 'blue', 'Wboth': 'green'}
            
            for param_type in df['param_type'].unique():
                subset = df[df['param_type'] == param_type]
                plt.scatter(subset['delta_ppl'], subset['delta_auc'], 
                           label=param_type, s=150, alpha=0.7, c=colors[param_type])
            
            plt.xlabel('Î”PPL', fontsize=14)
            plt.ylabel('Î”AUC', fontsize=14)
            plt.title('PRM Phase Diagram (A800 GPU)', fontsize=16)
            plt.legend(fontsize=12)
            plt.grid(True, alpha=0.3)
            plt.savefig(output_dir / 'prm_phase_diagram_gpu.png', dpi=300, bbox_inches='tight')
            plt.close()
            logger.info("âœ… PRMç›¸å›¾å·²ä¿å­˜")
        
    except Exception as e:
        logger.error(f"å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}")

def save_results_gpu(prm_results, umap_labels, sil_score, wfunc_count, wsens_count, wboth_count, hessian_data, output_dir):
    """ä¿å­˜GPUå®éªŒç»“æœ"""
    logger.info("ä¿å­˜å®éªŒç»“æœ...")
    
    try:
        # ä¿å­˜JSONç»“æœ
        results = {
            "timestamp": str(pd.Timestamp.now()),
            "experiment_type": "a800_gpu_optimized",
            "gpu_device": torch.cuda.get_device_name() if torch.cuda.is_available() else "CPU",
            "gpu_memory_used": torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0,
            "prm_results": prm_results,
            "umap_silhouette_score": sil_score,
            "num_clusters": len(set(umap_labels)) if umap_labels is not None else 0,
            "parameter_counts": {
                "Wfunc": wfunc_count,
                "Wsens": wsens_count,
                "Wboth": wboth_count
            },
            "hessian_data_size": len(hessian_data) if hessian_data is not None else 0,
            "gpu_optimized": True
        }
        
        with open(output_dir / "a800_gpu_results.json", "w") as f:
            json.dump(results, f, indent=2, default=str)
        
        # ä¿å­˜CSVç»“æœ
        if prm_results:
            df = pd.DataFrame(prm_results)
            df.to_csv(output_dir / "a800_gpu_results.csv", index=False)
        
        logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ A800 GPUå®éªŒå¼€å§‹")
    logger.info("=" * 50)
    
    # æ£€æŸ¥GPUå†…å­˜
    if not check_gpu_memory():
        logger.error("GPUå†…å­˜ä¸è¶³")
        return 1
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("prm_outputs")
    output_dir.mkdir(exist_ok=True)
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        model, tokenizer = load_model_gpu_optimized()
        if model is None:
            return 1
        
        # 2. å‡†å¤‡æ•°æ®
        inputs = prepare_gpu_data(tokenizer)
        if inputs is None:
            return 1
        
        # 3. è®¡ç®—Hessian
        hessian_data = compute_hessian_gpu_optimized(model, inputs)
        if hessian_data is None:
            return 1
        
        # 4. è®¡ç®—æ¿€æ´»ç¨€ç–åº¦
        sparsity = compute_activation_sparsity_gpu(model, inputs)
        
        # 5. UMAPå¯è§†åŒ–
        umap_embedding, umap_labels, sil_score = perform_umap_visualization_gpu(hessian_data, sparsity)
        
        # 6. å‚æ•°åˆ’åˆ†
        wfunc_count, wsens_count, wboth_count = partition_parameters_gpu(hessian_data, sparsity)
        
        # 7. PRMå®éªŒ
        prm_results = perform_prm_experiment_gpu(model, tokenizer, inputs)
        
        # 8. åˆ›å»ºå¯è§†åŒ–
        create_visualizations_gpu(umap_embedding, umap_labels, prm_results, output_dir)
        
        # 9. ä¿å­˜ç»“æœ
        save_results_gpu(prm_results, umap_labels, sil_score, wfunc_count, wsens_count, wboth_count, hessian_data, output_dir)
        
        logger.info("âœ… A800 GPUå®éªŒå®Œæˆ!")
        logger.info("æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
        files = list(output_dir.glob("*"))
        if files:
            logger.info("ç”Ÿæˆçš„æ–‡ä»¶:")
            for f in files:
                logger.info(f"  - {f.name}")
        
        return 0
        
    except Exception as e:
        logger.error(f"å®éªŒå¤±è´¥: {e}")
        return 1
    finally:
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"è‡´å‘½é”™è¯¯: {e}")
        sys.exit(1)
