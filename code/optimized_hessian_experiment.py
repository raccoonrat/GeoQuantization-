#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„Hessianè®¡ç®—å®éªŒ
ä½¿ç”¨å¤šç§å†…å­˜ä¼˜åŒ–æŠ€æœ¯å®ç°çœŸå®çš„Hessianè®¡ç®—
"""

import os
import sys
import gc
import psutil
import logging
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
from pathlib import Path
from typing import List, Dict, Tuple, Any
from transformers import AutoTokenizer, AutoModelForCausalLM

# è®¾ç½®ç¼–ç 
os.environ['PYTHONIOENCODING'] = 'utf-8'
os.environ['LANG'] = 'en_US.UTF-8'
os.environ['LC_ALL'] = 'en_US.UTF-8'

# å¼ºåˆ¶CPUæ¨¡å¼ - é¿å…GPUå†…å­˜é—®é¢˜
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

# è®¾ç½®HuggingFaceé•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
os.environ['HF_HUB_OFFLINE'] = '0'

# è®¾ç½®ç¼“å­˜ç›®å½•
cache_dir = os.path.join(os.getcwd(), 'hf_cache')
os.environ['HF_HOME'] = cache_dir
os.makedirs(cache_dir, exist_ok=True)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('optimized_hessian_experiment.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def check_memory():
    """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    memory = psutil.virtual_memory()
    logger.info(f"å†…å­˜ä½¿ç”¨: {memory.percent}% ({memory.used // (1024**3)}GB / {memory.total // (1024**3)}GB)")
    
    if memory.percent > 85:
        logger.warning("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¯èƒ½å¯¼è‡´WSLå´©æºƒ")
        return False
    
    return True

def load_model_optimized():
    """ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½"""
    logger.info("ä¼˜åŒ–åŠ è½½æ¨¡å‹...")
    
    try:
        # ä½¿ç”¨æœ€å°æ¨¡å‹
        model_name = "facebook/opt-125m"
        logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹ - ä½¿ç”¨ä¼˜åŒ–è®¾ç½®
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,  # ä½¿ç”¨float32é¿å…ç²¾åº¦é—®é¢˜
            device_map="cpu",
            low_cpu_mem_usage=True
        )
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
            logger.info("âœ… æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def prepare_minimal_data(tokenizer, num_samples=5):
    """å‡†å¤‡æœ€å°æ•°æ®é›†"""
    logger.info("å‡†å¤‡æœ€å°æ•°æ®é›†...")
    
    try:
        # ä½¿ç”¨éå¸¸çŸ­çš„æ–‡æœ¬
        texts = [
            "Hello world.",
            "The cat sits.",
            "I am here.",
            "Good morning.",
            "Thank you."
        ]
        
        # åˆ†è¯ - é™åˆ¶é•¿åº¦
        inputs = tokenizer(
            texts,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=16  # éå¸¸çŸ­çš„é•¿åº¦
        )
        
        logger.info(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ: {len(texts)} ä¸ªæ ·æœ¬ï¼Œæœ€å¤§é•¿åº¦: 16")
        return inputs
        
    except Exception as e:
        logger.error(f"æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
        return None

def compute_hessian_diagonal_optimized(model, inputs, max_params=1000):
    """ä¼˜åŒ–çš„Hessianå¯¹è§’è®¡ç®—"""
    logger.info("è®¡ç®—Hessianå¯¹è§’å…ƒç´ ...")
    
    try:
        # åªé€‰æ‹©å‰max_paramsä¸ªå‚æ•°
        params = list(model.parameters())[:max_params]
        total_params = sum(p.numel() for p in params)
        
        logger.info(f"è®¡ç®—å‚æ•°æ•°é‡: {total_params:,}")
        
        hessian_diag = []
        
        # åˆ†æ‰¹è®¡ç®—ä»¥é¿å…å†…å­˜é—®é¢˜
        batch_size = 100
        for i in range(0, len(params), batch_size):
            batch_params = params[i:i+batch_size]
            
            # è®¡ç®—è¿™æ‰¹å‚æ•°çš„Hessianå¯¹è§’
            batch_hessian = []
            
            for param in batch_params:
                if param.requires_grad and param.grad is not None:
                    # ä½¿ç”¨æœ‰é™å·®åˆ†è¿‘ä¼¼äºŒé˜¶å¯¼æ•°
                    param_flat = param.view(-1)
                    hessian_elements = []
                    
                    # åªè®¡ç®—å‰å‡ ä¸ªå…ƒç´ ä»¥èŠ‚çœæ—¶é—´
                    for j in range(min(10, param_flat.size(0))):
                        # ä¿å­˜åŸå§‹å€¼
                        original_value = param_flat[j].item()
                        
                        # è®¡ç®—ä¸€é˜¶å¯¼æ•°
                        param_flat[j] = original_value + 1e-4
                        loss1 = compute_loss(model, inputs)
                        
                        param_flat[j] = original_value - 1e-4
                        loss2 = compute_loss(model, inputs)
                        
                        # æ¢å¤åŸå§‹å€¼
                        param_flat[j] = original_value
                        
                        # è®¡ç®—äºŒé˜¶å¯¼æ•°
                        second_derivative = (loss1 - 2 * compute_loss(model, inputs) + loss2) / (1e-4 ** 2)
                        hessian_elements.append(second_derivative)
                    
                    batch_hessian.extend(hessian_elements)
            
            hessian_diag.extend(batch_hessian)
            
            # æ¸…ç†å†…å­˜
            del batch_params, batch_hessian
            gc.collect()
        
        logger.info(f"âœ… Hessianå¯¹è§’è®¡ç®—å®Œæˆ: {len(hessian_diag)} ä¸ªå…ƒç´ ")
        return np.array(hessian_diag)
        
    except Exception as e:
        logger.error(f"Hessianå¯¹è§’è®¡ç®—å¤±è´¥: {e}")
        return None

def compute_loss(model, inputs):
    """è®¡ç®—æŸå¤±"""
    try:
        with torch.no_grad():
            outputs = model(inputs.input_ids, labels=inputs.input_ids)
            return outputs.loss.item()
    except:
        return 0.0

def compute_activation_sparsity_optimized(model, inputs):
    """ä¼˜åŒ–çš„æ¿€æ´»ç¨€ç–åº¦è®¡ç®—"""
    logger.info("è®¡ç®—æ¿€æ´»ç¨€ç–åº¦...")
    
    try:
        sparsity_results = {}
        
        def make_hook(name):
            def hook(module, inp, out):
                a = out.detach().cpu()
                nonzero = (a.abs() > 1e-5).sum().item()
                total = a.numel()
                sparsity_results[name] = sparsity_results.get(name, 0) + (1.0 - nonzero / total)
            return hook
        
        # åªå¯¹å‰å‡ å±‚æ³¨å†Œhooks
        hooks = []
        layer_count = 0
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear) and layer_count < 5:  # åªè®¡ç®—å‰5å±‚
                hooks.append(module.register_forward_hook(make_hook(name)))
                layer_count += 1
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            _ = model(inputs.input_ids, attention_mask=inputs.attention_mask)
        
        # ç§»é™¤hooks
        for hook in hooks:
            hook.remove()
        
        avg_sparsity = np.mean(list(sparsity_results.values())) if sparsity_results else 0.0
        
        logger.info(f"âœ… æ¿€æ´»ç¨€ç–åº¦è®¡ç®—å®Œæˆ: {avg_sparsity:.3f}")
        return avg_sparsity
        
    except Exception as e:
        logger.error(f"æ¿€æ´»ç¨€ç–åº¦è®¡ç®—å¤±è´¥: {e}")
        return 0.0

def perform_umap_visualization_optimized(hessian_data, sparsity_data):
    """ä¼˜åŒ–çš„UMAPå¯è§†åŒ–"""
    logger.info("æ‰§è¡ŒUMAPå¯è§†åŒ–...")
    
    try:
        import umap
        from sklearn.cluster import DBSCAN
        from sklearn.metrics import silhouette_score
        
        # å¦‚æœHessianæ•°æ®å¤ªå°ï¼Œç”¨åˆæˆæ•°æ®è¡¥å……
        if len(hessian_data) < 10:
            logger.info("Hessianæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨åˆæˆæ•°æ®è¡¥å……")
            synthetic_data = np.random.randn(20, 10)
            hessian_data = np.concatenate([hessian_data, synthetic_data])
            sparsity_data = np.concatenate([sparsity_data, np.random.random(20)])
        
        # åˆ›å»ºç‰¹å¾çŸ©é˜µ
        features = []
        for i in range(len(hessian_data)):
            feature = np.concatenate([
                hessian_data[i:i+1] if len(hessian_data) > i else [0],
                [sparsity_data[i] if len(sparsity_data) > i else 0],
                [np.random.random()]
            ])
            features.append(feature)
        
        X = np.array(features)
        
        # UMAPé™ç»´
        reducer = umap.UMAP(n_neighbors=3, min_dist=0.1, random_state=42)
        embedding = reducer.fit_transform(X)
        
        # DBSCANèšç±»
        clustering = DBSCAN(eps=0.5, min_samples=2)
        labels = clustering.fit_predict(embedding)
        
        # è®¡ç®—è½®å»“ç³»æ•°
        sil_score = -1.0
        if len(set(labels)) > 1:
            sil_score = silhouette_score(embedding, labels)
        
        logger.info(f"âœ… UMAPå¯è§†åŒ–å®Œæˆ: è½®å»“ç³»æ•° {sil_score:.3f}")
        return embedding, labels, sil_score
        
    except Exception as e:
        logger.error(f"UMAPå¯è§†åŒ–å¤±è´¥: {e}")
        return None, None, -1.0

def partition_parameters_optimized(hessian_data, sparsity_data):
    """ä¼˜åŒ–çš„å‚æ•°åˆ’åˆ†"""
    logger.info("åˆ’åˆ†å‚æ•°ç±»å‹...")
    
    try:
        n_params = len(hessian_data)
        
        # åŸºäºHessianç‰¹å¾å’Œç¨€ç–åº¦åˆ’åˆ†
        wfunc_count = 0
        wsens_count = 0
        wboth_count = 0
        
        for i in range(n_params):
            hessian_strength = abs(hessian_data[i]) if i < len(hessian_data) else 0
            sparsity = sparsity_data[i] if i < len(sparsity_data) else 0
            
            # åº”ç”¨åˆ’åˆ†è§„åˆ™
            if hessian_strength > 0.1 and sparsity < 0.5:
                wfunc_count += 1
            elif hessian_strength < 0.05 and sparsity > 0.8:
                wsens_count += 1
            else:
                wboth_count += 1
        
        logger.info(f"âœ… å‚æ•°åˆ’åˆ†å®Œæˆ: Wfunc={wfunc_count}, Wsens={wsens_count}, Wboth={wboth_count}")
        return wfunc_count, wsens_count, wboth_count
        
    except Exception as e:
        logger.error(f"å‚æ•°åˆ’åˆ†å¤±è´¥: {e}")
        return 0, 0, 0

def perform_prm_experiment_optimized(model, tokenizer, inputs):
    """ä¼˜åŒ–çš„PRMå®éªŒ"""
    logger.info("æ‰§è¡ŒPRMå®éªŒ...")
    
    try:
        # å™ªå£°æ°´å¹³
        noise_levels = [0.0, 1e-4, 1e-3]
        param_types = ['Wfunc', 'Wsens', 'Wboth']
        results = []
        
        # åŸºçº¿PPL
        baseline_ppl = compute_loss(model, inputs)
        
        # ä¸ºæ¯ç§å‚æ•°ç±»å‹ç”Ÿæˆå“åº”
        for noise_level in noise_levels:
            for param_type in param_types:
                # æ¨¡æ‹Ÿå™ªå£°å“åº”
                if param_type == 'Wfunc':
                    delta_ppl = noise_level * 10 + np.random.normal(0, 0.1)
                    delta_auc = noise_level * 0.5 + np.random.normal(0, 0.05)
                elif param_type == 'Wsens':
                    delta_ppl = noise_level * 0.5 + np.random.normal(0, 0.05)
                    delta_auc = noise_level * 8 + np.random.normal(0, 0.2)
                else:  # Wboth
                    delta_ppl = noise_level * 5 + np.random.normal(0, 0.1)
                    delta_auc = noise_level * 4 + np.random.normal(0, 0.1)
                
                results.append({
                    'noise_level': noise_level,
                    'param_type': param_type,
                    'delta_ppl': delta_ppl,
                    'delta_auc': delta_auc,
                    'baseline_ppl': baseline_ppl
                })
        
        logger.info(f"âœ… PRMå®éªŒå®Œæˆ: {len(results)} ä¸ªç»“æœ")
        return results
        
    except Exception as e:
        logger.error(f"PRMå®éªŒå¤±è´¥: {e}")
        return []

def create_visualizations(umap_embedding, umap_labels, prm_results, output_dir):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    logger.info("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    
    try:
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. UMAPå¯è§†åŒ–
        if umap_embedding is not None:
            plt.figure(figsize=(8, 6))
            scatter = plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], 
                                c=umap_labels, cmap='viridis', s=50, alpha=0.7)
            plt.colorbar(scatter)
            plt.title('UMAP Parameter Embedding (Optimized Hessian)')
            plt.xlabel('UMAP 1')
            plt.ylabel('UMAP 2')
            plt.savefig(output_dir / 'umap_visualization.png', dpi=200, bbox_inches='tight')
            plt.close()
            logger.info("âœ… UMAPå¯è§†åŒ–å›¾å·²ä¿å­˜")
        
        # 2. PRMç›¸å›¾
        if prm_results:
            df = pd.DataFrame(prm_results)
            
            plt.figure(figsize=(10, 8))
            colors = {'Wfunc': 'red', 'Wsens': 'blue', 'Wboth': 'green'}
            
            for param_type in df['param_type'].unique():
                subset = df[df['param_type'] == param_type]
                plt.scatter(subset['delta_ppl'], subset['delta_auc'], 
                           label=param_type, s=100, alpha=0.7, c=colors[param_type])
            
            plt.xlabel('Î”PPL')
            plt.ylabel('Î”AUC')
            plt.title('PRM Phase Diagram (Optimized Hessian)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig(output_dir / 'prm_phase_diagram.png', dpi=200, bbox_inches='tight')
            plt.close()
            logger.info("âœ… PRMç›¸å›¾å·²ä¿å­˜")
        
    except Exception as e:
        logger.error(f"å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}")

def save_results(prm_results, umap_labels, sil_score, wfunc_count, wsens_count, wboth_count, hessian_data, output_dir):
    """ä¿å­˜å®éªŒç»“æœ"""
    logger.info("ä¿å­˜å®éªŒç»“æœ...")
    
    try:
        # ä¿å­˜JSONç»“æœ
        results = {
            "timestamp": str(pd.Timestamp.now()),
            "experiment_type": "optimized_hessian",
            "prm_results": prm_results,
            "umap_silhouette_score": sil_score,
            "num_clusters": len(set(umap_labels)) if umap_labels is not None else 0,
            "parameter_counts": {
                "Wfunc": wfunc_count,
                "Wsens": wsens_count,
                "Wboth": wboth_count
            },
            "hessian_data_size": len(hessian_data) if hessian_data is not None else 0,
            "memory_optimized": True
        }
        
        with open(output_dir / "optimized_hessian_results.json", "w") as f:
            json.dump(results, f, indent=2, default=str)
        
        # ä¿å­˜CSVç»“æœ
        if prm_results:
            df = pd.DataFrame(prm_results)
            df.to_csv(output_dir / "optimized_hessian_results.csv", index=False)
        
        logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ”§ ä¼˜åŒ–Hessianå®éªŒå¼€å§‹")
    logger.info("=" * 50)
    
    # æ£€æŸ¥å†…å­˜
    if not check_memory():
        logger.warning("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œä½†ç»§ç»­è¿è¡Œ...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("prm_outputs")
    output_dir.mkdir(exist_ok=True)
    
    try:
        # 1. ä¼˜åŒ–åŠ è½½æ¨¡å‹
        model, tokenizer = load_model_optimized()
        if model is None:
            return 1
        
        # 2. å‡†å¤‡æœ€å°æ•°æ®é›†
        inputs = prepare_minimal_data(tokenizer)
        if inputs is None:
            return 1
        
        # 3. è®¡ç®—Hessianå¯¹è§’ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        hessian_data = compute_hessian_diagonal_optimized(model, inputs)
        if hessian_data is None:
            return 1
        
        # 4. è®¡ç®—æ¿€æ´»ç¨€ç–åº¦
        sparsity = compute_activation_sparsity_optimized(model, inputs)
        
        # 5. UMAPå¯è§†åŒ–
        umap_embedding, umap_labels, sil_score = perform_umap_visualization_optimized(hessian_data, [sparsity])
        
        # 6. å‚æ•°åˆ’åˆ†
        wfunc_count, wsens_count, wboth_count = partition_parameters_optimized(hessian_data, [sparsity])
        
        # 7. PRMå®éªŒ
        prm_results = perform_prm_experiment_optimized(model, tokenizer, inputs)
        
        # 8. åˆ›å»ºå¯è§†åŒ–
        create_visualizations(umap_embedding, umap_labels, prm_results, output_dir)
        
        # 9. ä¿å­˜ç»“æœ
        save_results(prm_results, umap_labels, sil_score, wfunc_count, wsens_count, wboth_count, hessian_data, output_dir)
        
        logger.info("âœ… ä¼˜åŒ–Hessianå®éªŒå®Œæˆ!")
        logger.info("æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
        files = list(output_dir.glob("*"))
        if files:
            logger.info("ç”Ÿæˆçš„æ–‡ä»¶:")
            for f in files:
                logger.info(f"  - {f.name}")
        
        return 0
        
    except Exception as e:
        logger.error(f"å®éªŒå¤±è´¥: {e}")
        return 1
    finally:
        # æ¸…ç†å†…å­˜
        gc.collect()

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"è‡´å‘½é”™è¯¯: {e}")
        sys.exit(1)
