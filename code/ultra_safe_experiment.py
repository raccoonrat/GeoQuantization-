#!/usr/bin/env python3
"""
è¶…å®‰å…¨å®éªŒè„šæœ¬ - é¿å…Hessianè®¡ç®—å¯¼è‡´çš„å†…å­˜é—®é¢˜
ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–ç»“æœ
"""

import os
import sys
import gc
import psutil
import logging
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import List, Dict, Tuple, Any

# è®¾ç½®ç¼–ç 
os.environ['PYTHONIOENCODING'] = 'utf-8'
os.environ['LANG'] = 'en_US.UTF-8'
os.environ['LC_ALL'] = 'en_US.UTF-8'

# å¼ºåˆ¶CPUæ¨¡å¼ - é¿å…GPUå†…å­˜é—®é¢˜
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:16'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

# è®¾ç½®HuggingFaceé•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
os.environ['HF_HUB_OFFLINE'] = '0'

# è®¾ç½®ç¼“å­˜ç›®å½•
cache_dir = os.path.join(os.getcwd(), 'hf_cache')
os.environ['HF_HOME'] = cache_dir
os.makedirs(cache_dir, exist_ok=True)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ultra_safe_experiment.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def check_memory():
    """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    memory = psutil.virtual_memory()
    logger.info(f"å†…å­˜ä½¿ç”¨: {memory.percent}% ({memory.used // (1024**3)}GB / {memory.total // (1024**3)}GB)")
    
    if memory.percent > 80:
        logger.warning("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¯èƒ½å¯¼è‡´WSLå´©æºƒ")
        return False
    
    return True

def safe_model_test():
    """å®‰å…¨æ¨¡å‹æµ‹è¯• - æœ€å°åŒ–å†…å­˜ä½¿ç”¨"""
    logger.info("å®‰å…¨æ¨¡å‹æµ‹è¯•...")
    
    try:
        import torch
        from transformers import AutoTokenizer
        
        # åªåŠ è½½tokenizerï¼Œä¸åŠ è½½æ¨¡å‹
        model_name = "facebook/opt-125m"
        logger.info(f"åŠ è½½tokenizer: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ç®€å•æµ‹è¯•
        test_text = "Hello, world!"
        inputs = tokenizer(test_text, return_tensors="pt")
        
        logger.info("âœ… Tokenizeræµ‹è¯•æˆåŠŸ!")
        logger.info(f"è¾“å…¥å½¢çŠ¶: {inputs.input_ids.shape}")
        
        # æ¸…ç†å†…å­˜
        del tokenizer, inputs
        gc.collect()
        
        return True
        
    except Exception as e:
        logger.error(f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def generate_synthetic_hessian_data():
    """ç”ŸæˆåˆæˆHessianæ•°æ® - é¿å…å®é™…è®¡ç®—"""
    logger.info("ç”ŸæˆåˆæˆHessianæ•°æ®...")
    
    try:
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„Hessianç‰¹å¾æ•°æ®
        np.random.seed(42)
        n_samples = 20
        n_features = 10
        
        # æ¨¡æ‹ŸHessianç‰¹å¾å‘é‡
        hessian_data = np.random.randn(n_samples, n_features)
        
        # æ·»åŠ ä¸€äº›ç»“æ„
        hessian_data[:5] += 2.0  # å‰5ä¸ªæ ·æœ¬æœ‰æ›´é«˜çš„ç‰¹å¾å€¼
        hessian_data[5:10] -= 1.0  # ä¸­é—´5ä¸ªæ ·æœ¬æœ‰ä¸­ç­‰ç‰¹å¾å€¼
        hessian_data[10:] *= 0.5  # å10ä¸ªæ ·æœ¬æœ‰è¾ƒä½ç‰¹å¾å€¼
        
        logger.info(f"âœ… åˆæˆHessianæ•°æ®ç”Ÿæˆå®Œæˆ: {hessian_data.shape}")
        return hessian_data
        
    except Exception as e:
        logger.error(f"åˆæˆHessianæ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
        return None

def generate_synthetic_sparsity_data():
    """ç”Ÿæˆåˆæˆç¨€ç–åº¦æ•°æ®"""
    logger.info("ç”Ÿæˆåˆæˆç¨€ç–åº¦æ•°æ®...")
    
    try:
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„æ¿€æ´»ç¨€ç–åº¦
        np.random.seed(42)
        
        # ä¸åŒç±»å‹çš„å‚æ•°æœ‰ä¸åŒçš„ç¨€ç–åº¦æ¨¡å¼
        wfunc_sparsity = np.random.uniform(0.1, 0.4, 5)  # åŠŸèƒ½å‹ï¼šä½ç¨€ç–åº¦
        wsens_sparsity = np.random.uniform(0.8, 0.95, 5)  # æ•æ„Ÿå‹ï¼šé«˜ç¨€ç–åº¦
        wboth_sparsity = np.random.uniform(0.4, 0.7, 10)  # æ··åˆå‹ï¼šä¸­ç­‰ç¨€ç–åº¦
        
        all_sparsity = np.concatenate([wfunc_sparsity, wsens_sparsity, wboth_sparsity])
        
        logger.info(f"âœ… åˆæˆç¨€ç–åº¦æ•°æ®ç”Ÿæˆå®Œæˆ: {len(all_sparsity)} ä¸ªæ ·æœ¬")
        return all_sparsity
        
    except Exception as e:
        logger.error(f"åˆæˆç¨€ç–åº¦æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
        return None

def perform_umap_visualization(hessian_data, sparsity_data):
    """æ‰§è¡ŒUMAPå¯è§†åŒ–"""
    logger.info("æ‰§è¡ŒUMAPå¯è§†åŒ–...")
    
    try:
        import umap
        from sklearn.cluster import DBSCAN
        from sklearn.metrics import silhouette_score
        
        # åˆ›å»ºç‰¹å¾çŸ©é˜µ
        features = []
        for i in range(len(hessian_data)):
            feature = np.concatenate([
                hessian_data[i],
                [sparsity_data[i]],
                [np.random.random()]  # æ¨¡æ‹Ÿæ›²ç‡
            ])
            features.append(feature)
        
        X = np.array(features)
        
        # UMAPé™ç»´
        reducer = umap.UMAP(n_neighbors=5, min_dist=0.1, random_state=42)
        embedding = reducer.fit_transform(X)
        
        # DBSCANèšç±»
        clustering = DBSCAN(eps=0.5, min_samples=2)
        labels = clustering.fit_predict(embedding)
        
        # è®¡ç®—è½®å»“ç³»æ•°
        sil_score = -1.0
        if len(set(labels)) > 1:
            sil_score = silhouette_score(embedding, labels)
        
        logger.info(f"âœ… UMAPå¯è§†åŒ–å®Œæˆ: è½®å»“ç³»æ•° {sil_score:.3f}")
        return embedding, labels, sil_score
        
    except Exception as e:
        logger.error(f"UMAPå¯è§†åŒ–å¤±è´¥: {e}")
        return None, None, -1.0

def partition_parameters_synthetic(hessian_data, sparsity_data):
    """åŸºäºåˆæˆæ•°æ®åˆ’åˆ†å‚æ•°ç±»å‹"""
    logger.info("åˆ’åˆ†å‚æ•°ç±»å‹...")
    
    try:
        n_params = len(hessian_data)
        
        # åŸºäºHessianç‰¹å¾å’Œç¨€ç–åº¦åˆ’åˆ†
        wfunc_indices = []
        wsens_indices = []
        wboth_indices = []
        
        for i in range(n_params):
            # è®¡ç®—Hessianç‰¹å¾å¼ºåº¦ï¼ˆå‰å‡ ä¸ªç‰¹å¾å€¼çš„å’Œï¼‰
            hessian_strength = np.sum(hessian_data[i][:3])
            sparsity = sparsity_data[i]
            
            # åº”ç”¨åˆ’åˆ†è§„åˆ™
            if hessian_strength > 1.0 and sparsity < 0.5:
                wfunc_indices.append(i)
            elif hessian_strength < -0.5 and sparsity > 0.8:
                wsens_indices.append(i)
            else:
                wboth_indices.append(i)
        
        wfunc_count = len(wfunc_indices)
        wsens_count = len(wsens_indices)
        wboth_count = len(wboth_indices)
        
        logger.info(f"âœ… å‚æ•°åˆ’åˆ†å®Œæˆ: Wfunc={wfunc_count}, Wsens={wsens_count}, Wboth={wboth_count}")
        return wfunc_count, wsens_count, wboth_count, wfunc_indices, wsens_indices, wboth_indices
        
    except Exception as e:
        logger.error(f"å‚æ•°åˆ’åˆ†å¤±è´¥: {e}")
        return 0, 0, 0, [], [], []

def perform_synthetic_prm_experiment():
    """æ‰§è¡ŒåˆæˆPRMå®éªŒ"""
    logger.info("æ‰§è¡ŒåˆæˆPRMå®éªŒ...")
    
    try:
        # å™ªå£°æ°´å¹³
        noise_levels = [0.0, 1e-4, 1e-3]
        param_types = ['Wfunc', 'Wsens', 'Wboth']
        results = []
        
        # ä¸ºæ¯ç§å‚æ•°ç±»å‹ç”Ÿæˆä¸åŒçš„å“åº”æ¨¡å¼
        response_patterns = {
            'Wfunc': {'ppl_sensitivity': 2.0, 'auc_sensitivity': 0.1},  # ä¸»è¦å½±å“PPL
            'Wsens': {'ppl_sensitivity': 0.1, 'auc_sensitivity': 1.5},  # ä¸»è¦å½±å“AUC
            'Wboth': {'ppl_sensitivity': 1.0, 'auc_sensitivity': 1.0}   # åŒæ—¶å½±å“ä¸¤è€…
        }
        
        for noise_level in noise_levels:
            for param_type in param_types:
                pattern = response_patterns[param_type]
                
                # ç”Ÿæˆç¬¦åˆé¢„æœŸçš„å“åº”
                delta_ppl = np.random.normal(
                    noise_level * pattern['ppl_sensitivity'] * 10,
                    noise_level * 0.5
                )
                delta_auc = np.random.normal(
                    noise_level * pattern['auc_sensitivity'] * 5,
                    noise_level * 0.2
                )
                
                results.append({
                    'noise_level': noise_level,
                    'param_type': param_type,
                    'delta_ppl': delta_ppl,
                    'delta_auc': delta_auc,
                    'baseline_ppl': 10.0 + np.random.normal(0, 0.5)
                })
        
        logger.info(f"âœ… åˆæˆPRMå®éªŒå®Œæˆ: {len(results)} ä¸ªç»“æœ")
        return results
        
    except Exception as e:
        logger.error(f"åˆæˆPRMå®éªŒå¤±è´¥: {e}")
        return []

def create_visualizations(umap_embedding, umap_labels, prm_results, output_dir):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    logger.info("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    
    try:
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. UMAPå¯è§†åŒ–
        if umap_embedding is not None:
            plt.figure(figsize=(8, 6))
            scatter = plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], 
                                c=umap_labels, cmap='viridis', s=50, alpha=0.7)
            plt.colorbar(scatter)
            plt.title('UMAP Parameter Embedding (Synthetic Data)')
            plt.xlabel('UMAP 1')
            plt.ylabel('UMAP 2')
            plt.savefig(output_dir / 'umap_visualization.png', dpi=200, bbox_inches='tight')
            plt.close()
            logger.info("âœ… UMAPå¯è§†åŒ–å›¾å·²ä¿å­˜")
        
        # 2. PRMç›¸å›¾
        if prm_results:
            df = pd.DataFrame(prm_results)
            
            plt.figure(figsize=(10, 8))
            colors = {'Wfunc': 'red', 'Wsens': 'blue', 'Wboth': 'green'}
            
            for param_type in df['param_type'].unique():
                subset = df[df['param_type'] == param_type]
                plt.scatter(subset['delta_ppl'], subset['delta_auc'], 
                           label=param_type, s=100, alpha=0.7, c=colors[param_type])
            
            plt.xlabel('Î”PPL')
            plt.ylabel('Î”AUC')
            plt.title('PRM Phase Diagram (Synthetic Data)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig(output_dir / 'prm_phase_diagram.png', dpi=200, bbox_inches='tight')
            plt.close()
            logger.info("âœ… PRMç›¸å›¾å·²ä¿å­˜")
            
            # 3. å™ªå£°å“åº”æ›²çº¿
            plt.figure(figsize=(12, 5))
            
            # PPLå“åº”
            plt.subplot(1, 2, 1)
            for param_type in df['param_type'].unique():
                subset = df[df['param_type'] == param_type]
                plt.plot(subset['noise_level'], subset['delta_ppl'], 
                        'o-', label=param_type, linewidth=2, markersize=6, c=colors[param_type])
            plt.xlabel('Noise Level')
            plt.ylabel('Î”PPL')
            plt.title('PPL Response to Noise')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # AUCå“åº”
            plt.subplot(1, 2, 2)
            for param_type in df['param_type'].unique():
                subset = df[df['param_type'] == param_type]
                plt.plot(subset['noise_level'], subset['delta_auc'], 
                        'o-', label=param_type, linewidth=2, markersize=6, c=colors[param_type])
            plt.xlabel('Noise Level')
            plt.ylabel('Î”AUC')
            plt.title('AUC Response to Noise')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(output_dir / 'noise_response_curves.png', dpi=200, bbox_inches='tight')
            plt.close()
            logger.info("âœ… å™ªå£°å“åº”æ›²çº¿å·²ä¿å­˜")
        
        # 4. ç»Ÿè®¡æ‘˜è¦å›¾
        if prm_results:
            df = pd.DataFrame(prm_results)
            
            plt.figure(figsize=(10, 6))
            summary = df.groupby('param_type').agg({
                'delta_ppl': ['mean', 'std'],
                'delta_auc': ['mean', 'std']
            }).round(4)
            
            # åˆ›å»ºçƒ­å›¾
            sns.heatmap(summary, annot=True, fmt='.4f', cmap='viridis')
            plt.title('Statistical Summary Heatmap (Synthetic Data)')
            plt.tight_layout()
            plt.savefig(output_dir / 'statistical_summary.png', dpi=200, bbox_inches='tight')
            plt.close()
            logger.info("âœ… ç»Ÿè®¡æ‘˜è¦å›¾å·²ä¿å­˜")
        
        # 5. å‚æ•°åˆ†å¸ƒå›¾
        if umap_embedding is not None and umap_labels is not None:
            plt.figure(figsize=(8, 6))
            
            # æ ¹æ®æ ‡ç­¾ç€è‰²
            unique_labels = np.unique(umap_labels)
            colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))
            
            for i, label in enumerate(unique_labels):
                mask = umap_labels == label
                plt.scatter(umap_embedding[mask, 0], umap_embedding[mask, 1], 
                           c=[colors[i]], label=f'Cluster {label}', s=50, alpha=0.7)
            
            plt.xlabel('UMAP 1')
            plt.ylabel('UMAP 2')
            plt.title('Parameter Clustering (Synthetic Data)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig(output_dir / 'parameter_clustering.png', dpi=200, bbox_inches='tight')
            plt.close()
            logger.info("âœ… å‚æ•°èšç±»å›¾å·²ä¿å­˜")
        
    except Exception as e:
        logger.error(f"å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}")

def save_results(prm_results, umap_labels, sil_score, wfunc_count, wsens_count, wboth_count, output_dir):
    """ä¿å­˜å®éªŒç»“æœ"""
    logger.info("ä¿å­˜å®éªŒç»“æœ...")
    
    try:
        # ä¿å­˜JSONç»“æœ
        results = {
            "timestamp": str(pd.Timestamp.now()),
            "experiment_type": "ultra_safe_synthetic",
            "prm_results": prm_results,
            "umap_silhouette_score": sil_score,
            "num_clusters": len(set(umap_labels)) if umap_labels is not None else 0,
            "parameter_counts": {
                "Wfunc": wfunc_count,
                "Wsens": wsens_count,
                "Wboth": wboth_count
            },
            "data_type": "synthetic",
            "memory_safe": True
        }
        
        with open(output_dir / "ultra_safe_results.json", "w") as f:
            json.dump(results, f, indent=2, default=str)
        
        # ä¿å­˜CSVç»“æœ
        if prm_results:
            df = pd.DataFrame(prm_results)
            df.to_csv(output_dir / "ultra_safe_results.csv", index=False)
        
        logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ›¡ï¸ è¶…å®‰å…¨å®éªŒå¼€å§‹ï¼ˆä½¿ç”¨åˆæˆæ•°æ®ï¼‰")
    logger.info("=" * 50)
    
    # æ£€æŸ¥å†…å­˜
    if not check_memory():
        logger.warning("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œä½†ç»§ç»­è¿è¡Œ...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("prm_outputs")
    output_dir.mkdir(exist_ok=True)
    
    try:
        # 1. å®‰å…¨æ¨¡å‹æµ‹è¯•ï¼ˆåªæµ‹è¯•tokenizerï¼‰
        if not safe_model_test():
            logger.error("æ¨¡å‹æµ‹è¯•å¤±è´¥")
            return 1
        
        # 2. ç”ŸæˆåˆæˆHessianæ•°æ®
        hessian_data = generate_synthetic_hessian_data()
        if hessian_data is None:
            return 1
        
        # 3. ç”Ÿæˆåˆæˆç¨€ç–åº¦æ•°æ®
        sparsity_data = generate_synthetic_sparsity_data()
        if sparsity_data is None:
            return 1
        
        # 4. UMAPå¯è§†åŒ–
        umap_embedding, umap_labels, sil_score = perform_umap_visualization(hessian_data, sparsity_data)
        
        # 5. å‚æ•°åˆ’åˆ†
        wfunc_count, wsens_count, wboth_count, wfunc_indices, wsens_indices, wboth_indices = partition_parameters_synthetic(hessian_data, sparsity_data)
        
        # 6. åˆæˆPRMå®éªŒ
        prm_results = perform_synthetic_prm_experiment()
        
        # 7. åˆ›å»ºå¯è§†åŒ–
        create_visualizations(umap_embedding, umap_labels, prm_results, output_dir)
        
        # 8. ä¿å­˜ç»“æœ
        save_results(prm_results, umap_labels, sil_score, wfunc_count, wsens_count, wboth_count, output_dir)
        
        logger.info("âœ… è¶…å®‰å…¨å®éªŒå®Œæˆ!")
        logger.info("æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆï¼ˆä½¿ç”¨åˆæˆæ•°æ®ï¼‰")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
        files = list(output_dir.glob("*"))
        if files:
            logger.info("ç”Ÿæˆçš„æ–‡ä»¶:")
            for f in files:
                logger.info(f"  - {f.name}")
        
        return 0
        
    except Exception as e:
        logger.error(f"å®éªŒå¤±è´¥: {e}")
        return 1
    finally:
        # æ¸…ç†å†…å­˜
        gc.collect()

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"è‡´å‘½é”™è¯¯: {e}")
        sys.exit(1)
