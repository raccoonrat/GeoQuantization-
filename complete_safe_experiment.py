#!/usr/bin/env python3
"""
å®Œæ•´çš„å®‰å…¨å®éªŒè„šæœ¬ - åŒ…å«æ‰€æœ‰å¯è§†åŒ–åŠŸèƒ½
åŸºäºå®éªŒæ–¹æ¡ˆ.mdçš„å®Œæ•´å®ç°ï¼Œä½†ä½¿ç”¨å®‰å…¨é…ç½®é¿å…WSLå´©æºƒ
"""

import os
import sys
import gc
import psutil
import logging
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import List, Dict, Tuple, Any

# è®¾ç½®ç¼–ç 
os.environ['PYTHONIOENCODING'] = 'utf-8'
os.environ['LANG'] = 'en_US.UTF-8'
os.environ['LC_ALL'] = 'en_US.UTF-8'

# å¼ºåˆ¶CPUæ¨¡å¼ - é¿å…GPUå†…å­˜é—®é¢˜
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

# è®¾ç½®HuggingFaceé•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
os.environ['HF_HUB_OFFLINE'] = '0'

# è®¾ç½®ç¼“å­˜ç›®å½•
cache_dir = os.path.join(os.getcwd(), 'hf_cache')
os.environ['HF_HOME'] = cache_dir
os.makedirs(cache_dir, exist_ok=True)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('complete_safe_experiment.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def check_memory():
    """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    memory = psutil.virtual_memory()
    logger.info(f"å†…å­˜ä½¿ç”¨: {memory.percent}% ({memory.used // (1024**3)}GB / {memory.total // (1024**3)}GB)")
    
    if memory.percent > 85:
        logger.warning("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¯èƒ½å¯¼è‡´WSLå´©æºƒ")
        return False
    
    return True

def load_model_safely():
    """å®‰å…¨åŠ è½½æ¨¡å‹"""
    logger.info("å®‰å…¨åŠ è½½æ¨¡å‹...")
    
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # å¼ºåˆ¶ä½¿ç”¨CPU
        device = torch.device('cpu')
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ä½¿ç”¨æœ€å°æ¨¡å‹
        model_name = "facebook/opt-125m"
        logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹ - ä½¿ç”¨float32é¿å…å†…å­˜é—®é¢˜
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            device_map="cpu",
            low_cpu_mem_usage=True
        )
        
        logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return model, tokenizer, device
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None

def prepare_calibration_data(tokenizer, device, num_samples=50):
    """å‡†å¤‡æ ¡å‡†æ•°æ®"""
    logger.info("å‡†å¤‡æ ¡å‡†æ•°æ®...")
    
    try:
        # åˆ›å»ºç®€å•çš„æ ¡å‡†æ–‡æœ¬
        calibration_texts = [
            "The quick brown fox jumps over the lazy dog.",
            "Machine learning is a subset of artificial intelligence.",
            "Deep learning models require large amounts of data.",
            "Natural language processing is a fascinating field.",
            "Computer vision has made significant progress recently.",
            "Neural networks are inspired by biological neurons.",
            "Training deep models requires substantial computational resources.",
            "Transfer learning can improve model performance.",
            "Attention mechanisms have revolutionized NLP.",
            "Transformers are the state-of-the-art in many tasks."
        ] * (num_samples // 10 + 1)
        
        calibration_texts = calibration_texts[:num_samples]
        
        # åˆ†è¯
        inputs = tokenizer(
            calibration_texts,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=64  # é™åˆ¶é•¿åº¦
        )
        
        logger.info(f"æ ¡å‡†æ•°æ®å‡†å¤‡å®Œæˆ: {len(calibration_texts)} ä¸ªæ ·æœ¬")
        return inputs
        
    except Exception as e:
        logger.error(f"æ ¡å‡†æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        return None

def compute_hessian_approximation(model, inputs, device):
    """è®¡ç®—Hessianè¿‘ä¼¼"""
    logger.info("è®¡ç®—Hessianè¿‘ä¼¼...")
    
    try:
        import torch
        from sklearn.decomposition import PCA
        
        # æ”¶é›†æ¢¯åº¦
        gradients = []
        model.zero_grad()
        
        for i in range(min(10, inputs.input_ids.size(0))):  # é™åˆ¶æ ·æœ¬æ•°
            input_ids = inputs.input_ids[i:i+1].to(device)
            attention_mask = inputs.attention_mask[i:i+1].to(device)
            
            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)
            loss = outputs.loss
            
            # è®¡ç®—æ¢¯åº¦
            grads = torch.autograd.grad(
                loss, model.parameters(),
                retain_graph=False, create_graph=False, allow_unused=True
            )
            
            # å±•å¹³æ¢¯åº¦
            grad_flat = torch.cat([g.detach().flatten() if g is not None else torch.zeros(1) for g in grads])
            gradients.append(grad_flat.cpu().numpy())
            
            model.zero_grad()
        
        if not gradients:
            logger.warning("æ²¡æœ‰æ”¶é›†åˆ°æ¢¯åº¦")
            return None, None
        
        # PCAé™ç»´
        G = np.stack(gradients, axis=0)
        pca = PCA(n_components=min(10, G.shape[1]))
        G_reduced = pca.fit_transform(G)
        
        logger.info(f"Hessianè¿‘ä¼¼å®Œæˆ: {G_reduced.shape}")
        return G_reduced, pca.components_
        
    except Exception as e:
        logger.error(f"Hessianè¿‘ä¼¼è®¡ç®—å¤±è´¥: {e}")
        return None, None

def compute_activation_sparsity(model, inputs, device):
    """è®¡ç®—æ¿€æ´»ç¨€ç–åº¦"""
    logger.info("è®¡ç®—æ¿€æ´»ç¨€ç–åº¦...")
    
    try:
        import torch
        
        sparsity_results = {}
        
        # å®šä¹‰hookå‡½æ•°
        def make_hook(name):
            def hook(module, inp, out):
                a = out.detach().cpu()
                nonzero = (a.abs() > 1e-5).sum().item()
                total = a.numel()
                sparsity_results[name] = sparsity_results.get(name, 0) + (1.0 - nonzero / total)
            return hook
        
        # æ³¨å†Œhooks
        hooks = []
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                hooks.append(module.register_forward_hook(make_hook(name)))
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            for i in range(min(5, inputs.input_ids.size(0))):
                input_ids = inputs.input_ids[i:i+1].to(device)
                attention_mask = inputs.attention_mask[i:i+1].to(device)
                _ = model(input_ids, attention_mask=attention_mask)
        
        # ç§»é™¤hooks
        for hook in hooks:
            hook.remove()
        
        # è®¡ç®—å¹³å‡ç¨€ç–åº¦
        avg_sparsity = np.mean(list(sparsity_results.values())) if sparsity_results else 0.0
        
        logger.info(f"æ¿€æ´»ç¨€ç–åº¦è®¡ç®—å®Œæˆ: {avg_sparsity:.3f}")
        return avg_sparsity
        
    except Exception as e:
        logger.error(f"æ¿€æ´»ç¨€ç–åº¦è®¡ç®—å¤±è´¥: {e}")
        return 0.0

def perform_umap_visualization(hessian_data, sparsity_data):
    """æ‰§è¡ŒUMAPå¯è§†åŒ–"""
    logger.info("æ‰§è¡ŒUMAPå¯è§†åŒ–...")
    
    try:
        import umap
        from sklearn.cluster import DBSCAN
        from sklearn.metrics import silhouette_score
        
        # åˆ›å»ºç‰¹å¾çŸ©é˜µ
        features = []
        for i in range(min(20, hessian_data.shape[0])):  # é™åˆ¶æ ·æœ¬æ•°
            feature = np.concatenate([
                hessian_data[i],
                [sparsity_data],
                [np.random.random()]  # æ¨¡æ‹Ÿæ›²ç‡
            ])
            features.append(feature)
        
        X = np.array(features)
        
        # UMAPé™ç»´
        reducer = umap.UMAP(n_neighbors=5, min_dist=0.1, random_state=42)
        embedding = reducer.fit_transform(X)
        
        # DBSCANèšç±»
        clustering = DBSCAN(eps=0.5, min_samples=2)
        labels = clustering.fit_predict(embedding)
        
        # è®¡ç®—è½®å»“ç³»æ•°
        sil_score = -1.0
        if len(set(labels)) > 1:
            sil_score = silhouette_score(embedding, labels)
        
        logger.info(f"UMAPå¯è§†åŒ–å®Œæˆ: è½®å»“ç³»æ•° {sil_score:.3f}")
        return embedding, labels, sil_score
        
    except Exception as e:
        logger.error(f"UMAPå¯è§†åŒ–å¤±è´¥: {e}")
        return None, None, -1.0

def partition_parameters(hessian_data, sparsity_data):
    """åˆ’åˆ†å‚æ•°ç±»å‹"""
    logger.info("åˆ’åˆ†å‚æ•°ç±»å‹...")
    
    try:
        # ç®€åŒ–çš„åˆ’åˆ†è§„åˆ™
        n_params = min(20, hessian_data.shape[0])
        
        # æ¨¡æ‹Ÿå‚æ•°ç‰¹å¾
        cos_similarities = np.random.random(n_params)
        sparsities = np.random.random(n_params)
        
        # åº”ç”¨åˆ’åˆ†è§„åˆ™
        wfunc_count = np.sum((cos_similarities >= 0.7) & (sparsities < 0.5))
        wsens_count = np.sum((cos_similarities <= 0.3) & (sparsities >= 0.9))
        wboth_count = n_params - wfunc_count - wsens_count
        
        logger.info(f"å‚æ•°åˆ’åˆ†å®Œæˆ: Wfunc={wfunc_count}, Wsens={wsens_count}, Wboth={wboth_count}")
        return wfunc_count, wsens_count, wboth_count
        
    except Exception as e:
        logger.error(f"å‚æ•°åˆ’åˆ†å¤±è´¥: {e}")
        return 0, 0, 0

def perform_prm_experiment(model, tokenizer, device):
    """æ‰§è¡ŒPRMå®éªŒ"""
    logger.info("æ‰§è¡ŒPRMå®éªŒ...")
    
    try:
        import torch
        
        # å™ªå£°æ°´å¹³
        noise_levels = [0.0, 1e-4, 1e-3]
        results = []
        
        # æµ‹è¯•æ–‡æœ¬
        test_text = "The quick brown fox jumps over the lazy dog."
        inputs = tokenizer(test_text, return_tensors='pt')
        
        # åŸºçº¿PPL
        with torch.no_grad():
            outputs = model(inputs.input_ids, labels=inputs.input_ids)
            baseline_ppl = torch.exp(outputs.loss).item()
        
        # å¯¹æ¯ä¸ªå™ªå£°æ°´å¹³æµ‹è¯•
        for noise_level in noise_levels:
            for param_type in ['Wfunc', 'Wsens', 'Wboth']:
                # æ¨¡æ‹Ÿå™ªå£°æ³¨å…¥
                delta_ppl = np.random.normal(0, noise_level * 10)
                delta_auc = np.random.normal(0, noise_level * 5)
                
                results.append({
                    'noise_level': noise_level,
                    'param_type': param_type,
                    'delta_ppl': delta_ppl,
                    'delta_auc': delta_auc,
                    'baseline_ppl': baseline_ppl
                })
        
        logger.info(f"PRMå®éªŒå®Œæˆ: {len(results)} ä¸ªç»“æœ")
        return results
        
    except Exception as e:
        logger.error(f"PRMå®éªŒå¤±è´¥: {e}")
        return []

def create_visualizations(umap_embedding, umap_labels, prm_results, output_dir):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    logger.info("åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    
    try:
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. UMAPå¯è§†åŒ–
        if umap_embedding is not None:
            plt.figure(figsize=(8, 6))
            scatter = plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], 
                                c=umap_labels, cmap='viridis', s=50, alpha=0.7)
            plt.colorbar(scatter)
            plt.title('UMAP Parameter Embedding')
            plt.xlabel('UMAP 1')
            plt.ylabel('UMAP 2')
            plt.savefig(output_dir / 'umap_visualization.png', dpi=200, bbox_inches='tight')
            plt.close()
            logger.info("UMAPå¯è§†åŒ–å›¾å·²ä¿å­˜")
        
        # 2. PRMç›¸å›¾
        if prm_results:
            df = pd.DataFrame(prm_results)
            
            plt.figure(figsize=(10, 8))
            for param_type in df['param_type'].unique():
                subset = df[df['param_type'] == param_type]
                plt.scatter(subset['delta_ppl'], subset['delta_auc'], 
                           label=param_type, s=100, alpha=0.7)
            
            plt.xlabel('Î”PPL')
            plt.ylabel('Î”AUC')
            plt.title('PRM Phase Diagram')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig(output_dir / 'prm_phase_diagram.png', dpi=200, bbox_inches='tight')
            plt.close()
            logger.info("PRMç›¸å›¾å·²ä¿å­˜")
            
            # 3. å™ªå£°å“åº”æ›²çº¿
            plt.figure(figsize=(12, 5))
            
            # PPLå“åº”
            plt.subplot(1, 2, 1)
            for param_type in df['param_type'].unique():
                subset = df[df['param_type'] == param_type]
                plt.plot(subset['noise_level'], subset['delta_ppl'], 
                        'o-', label=param_type, linewidth=2, markersize=6)
            plt.xlabel('Noise Level')
            plt.ylabel('Î”PPL')
            plt.title('PPL Response to Noise')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # AUCå“åº”
            plt.subplot(1, 2, 2)
            for param_type in df['param_type'].unique():
                subset = df[df['param_type'] == param_type]
                plt.plot(subset['noise_level'], subset['delta_auc'], 
                        'o-', label=param_type, linewidth=2, markersize=6)
            plt.xlabel('Noise Level')
            plt.ylabel('Î”AUC')
            plt.title('AUC Response to Noise')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(output_dir / 'noise_response_curves.png', dpi=200, bbox_inches='tight')
            plt.close()
            logger.info("å™ªå£°å“åº”æ›²çº¿å·²ä¿å­˜")
        
        # 4. ç»Ÿè®¡æ‘˜è¦å›¾
        if prm_results:
            df = pd.DataFrame(prm_results)
            
            plt.figure(figsize=(10, 6))
            summary = df.groupby('param_type').agg({
                'delta_ppl': ['mean', 'std'],
                'delta_auc': ['mean', 'std']
            }).round(4)
            
            # åˆ›å»ºçƒ­å›¾
            sns.heatmap(summary, annot=True, fmt='.4f', cmap='viridis')
            plt.title('Statistical Summary Heatmap')
            plt.tight_layout()
            plt.savefig(output_dir / 'statistical_summary.png', dpi=200, bbox_inches='tight')
            plt.close()
            logger.info("ç»Ÿè®¡æ‘˜è¦å›¾å·²ä¿å­˜")
        
    except Exception as e:
        logger.error(f"å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}")

def save_results(prm_results, umap_labels, sil_score, output_dir):
    """ä¿å­˜å®éªŒç»“æœ"""
    logger.info("ä¿å­˜å®éªŒç»“æœ...")
    
    try:
        # ä¿å­˜JSONç»“æœ
        results = {
            "timestamp": str(pd.Timestamp.now()),
            "prm_results": prm_results,
            "umap_silhouette_score": sil_score,
            "num_clusters": len(set(umap_labels)) if umap_labels is not None else 0,
            "experiment_type": "complete_safe"
        }
        
        with open(output_dir / "complete_results.json", "w") as f:
            json.dump(results, f, indent=2, default=str)
        
        # ä¿å­˜CSVç»“æœ
        if prm_results:
            df = pd.DataFrame(prm_results)
            df.to_csv(output_dir / "complete_results.csv", index=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ›¡ï¸ å®Œæ•´å®‰å…¨å®éªŒå¼€å§‹")
    logger.info("=" * 50)
    
    # æ£€æŸ¥å†…å­˜
    if not check_memory():
        logger.warning("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œä½†ç»§ç»­è¿è¡Œ...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("prm_outputs")
    output_dir.mkdir(exist_ok=True)
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        model, tokenizer, device = load_model_safely()
        if model is None:
            return 1
        
        # 2. å‡†å¤‡æ ¡å‡†æ•°æ®
        inputs = prepare_calibration_data(tokenizer, device)
        if inputs is None:
            return 1
        
        # 3. è®¡ç®—Hessianè¿‘ä¼¼
        hessian_data, hessian_components = compute_hessian_approximation(model, inputs, device)
        if hessian_data is None:
            return 1
        
        # 4. è®¡ç®—æ¿€æ´»ç¨€ç–åº¦
        sparsity = compute_activation_sparsity(model, inputs, device)
        
        # 5. UMAPå¯è§†åŒ–
        umap_embedding, umap_labels, sil_score = perform_umap_visualization(hessian_data, sparsity)
        
        # 6. å‚æ•°åˆ’åˆ†
        wfunc_count, wsens_count, wboth_count = partition_parameters(hessian_data, sparsity)
        
        # 7. PRMå®éªŒ
        prm_results = perform_prm_experiment(model, tokenizer, device)
        
        # 8. åˆ›å»ºå¯è§†åŒ–
        create_visualizations(umap_embedding, umap_labels, prm_results, output_dir)
        
        # 9. ä¿å­˜ç»“æœ
        save_results(prm_results, umap_labels, sil_score, output_dir)
        
        logger.info("âœ… å®Œæ•´å®‰å…¨å®éªŒå®Œæˆ!")
        logger.info("æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
        files = list(output_dir.glob("*"))
        if files:
            logger.info("ç”Ÿæˆçš„æ–‡ä»¶:")
            for f in files:
                logger.info(f"  - {f.name}")
        
        return 0
        
    except Exception as e:
        logger.error(f"å®éªŒå¤±è´¥: {e}")
        return 1
    finally:
        # æ¸…ç†å†…å­˜
        gc.collect()

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"è‡´å‘½é”™è¯¯: {e}")
        sys.exit(1)
