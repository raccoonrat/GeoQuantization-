#!/usr/bin/env python3
"""
Hessianå†…å­˜æ¶ˆè€—åˆ†æå·¥å…·
é¢„ä¼°ä¸åŒæ¨¡å‹å’Œé…ç½®ä¸‹çš„å†…å­˜éœ€æ±‚
"""

import os
import sys
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
import psutil
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_model_parameter_count(model_name):
    """è·å–æ¨¡å‹å‚æ•°æ•°é‡"""
    try:
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        del model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        return total_params, trainable_params
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹å‚æ•°å¤±è´¥: {e}")
        return None, None

def estimate_hessian_memory_usage(model_name, batch_size=1, sequence_length=128):
    """é¢„ä¼°Hessianè®¡ç®—çš„å†…å­˜ä½¿ç”¨"""
    logger.info(f"åˆ†ææ¨¡å‹: {model_name}")
    
    try:
        # è·å–æ¨¡å‹å‚æ•°æ•°é‡
        total_params, trainable_params = get_model_parameter_count(model_name)
        if total_params is None:
            return None
        
        logger.info(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
        logger.info(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
        
        # å†…å­˜è®¡ç®—ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰
        param_memory = total_params * 4  # float32 = 4 bytes
        
        # æ¢¯åº¦å†…å­˜ï¼ˆä¸å‚æ•°ç›¸åŒå¤§å°ï¼‰
        gradient_memory = trainable_params * 4
        
        # HessiançŸ©é˜µå†…å­˜ï¼ˆå¯¹äºäºŒé˜¶å¯¼æ•°ï¼‰
        # æ³¨æ„ï¼šå®Œæ•´HessiançŸ©é˜µæ˜¯ NÃ—Nï¼Œä½†é€šå¸¸åªè®¡ç®—å¯¹è§’æˆ–å—å¯¹è§’
        hessian_diagonal_memory = trainable_params * 4  # å¯¹è§’å…ƒç´ 
        hessian_full_memory = trainable_params * trainable_params * 4  # å®Œæ•´çŸ©é˜µ
        
        # æ¿€æ´»å†…å­˜ï¼ˆå‰å‘ä¼ æ’­ï¼‰
        # ä¼°ç®—ï¼šæ¯å±‚æ¿€æ´»å¤§å° â‰ˆ batch_size * sequence_length * hidden_size
        # è¿™é‡Œä½¿ç”¨ç»éªŒå…¬å¼
        estimated_hidden_size = min(4096, total_params // 100000)  # ç»éªŒä¼°ç®—
        activation_memory = batch_size * sequence_length * estimated_hidden_size * 4
        
        # ä¸­é—´è®¡ç®—å†…å­˜
        intermediate_memory = param_memory * 2  # ä¸­é—´å˜é‡
        
        # æ€»å†…å­˜ä¼°ç®—
        memory_breakdown = {
            'parameters': param_memory,
            'gradients': gradient_memory,
            'hessian_diagonal': hessian_diagonal_memory,
            'hessian_full': hessian_full_memory,
            'activations': activation_memory,
            'intermediate': intermediate_memory
        }
        
        # ä¸åŒç­–ç•¥çš„å†…å­˜éœ€æ±‚
        strategies = {
            'diagonal_hessian': param_memory + gradient_memory + hessian_diagonal_memory + activation_memory + intermediate_memory,
            'block_diagonal_hessian': param_memory + gradient_memory + hessian_diagonal_memory * 10 + activation_memory + intermediate_memory,
            'full_hessian': param_memory + gradient_memory + hessian_full_memory + activation_memory + intermediate_memory,
            'approximate_hessian': param_memory + gradient_memory + hessian_diagonal_memory + activation_memory + intermediate_memory * 0.5
        }
        
        return {
            'model_name': model_name,
            'total_params': total_params,
            'trainable_params': trainable_params,
            'memory_breakdown': memory_breakdown,
            'strategies': strategies
        }
        
    except Exception as e:
        logger.error(f"å†…å­˜åˆ†æå¤±è´¥: {e}")
        return None

def format_memory_size(bytes_size):
    """æ ¼å¼åŒ–å†…å­˜å¤§å°"""
    if bytes_size < 1024**3:
        return f"{bytes_size / (1024**2):.1f} MB"
    else:
        return f"{bytes_size / (1024**3):.1f} GB"

def analyze_multiple_models():
    """åˆ†æå¤šä¸ªæ¨¡å‹çš„å†…å­˜éœ€æ±‚"""
    models = [
        "facebook/opt-125m",
        "facebook/opt-350m", 
        "facebook/opt-1.3b",
        "facebook/opt-2.7b",
        "facebook/opt-6.7b"
    ]
    
    results = []
    
    for model_name in models:
        logger.info(f"\n{'='*60}")
        logger.info(f"åˆ†ææ¨¡å‹: {model_name}")
        logger.info(f"{'='*60}")
        
        result = estimate_hessian_memory_usage(model_name)
        if result:
            results.append(result)
            
            # æ˜¾ç¤ºç»“æœ
            logger.info(f"å‚æ•°æ•°é‡: {result['total_params']:,}")
            logger.info(f"å¯è®­ç»ƒå‚æ•°: {result['trainable_params']:,}")
            
            logger.info("\nå†…å­˜åˆ†è§£:")
            for key, value in result['memory_breakdown'].items():
                logger.info(f"  {key}: {format_memory_size(value)}")
            
            logger.info("\nä¸åŒç­–ç•¥çš„å†…å­˜éœ€æ±‚:")
            for strategy, memory in result['strategies'].items():
                logger.info(f"  {strategy}: {format_memory_size(memory)}")
    
    return results

def recommend_strategy(results):
    """æ¨èæœ€ä½³ç­–ç•¥"""
    logger.info(f"\n{'='*60}")
    logger.info("ç­–ç•¥æ¨è")
    logger.info(f"{'='*60}")
    
    # è·å–ç³»ç»Ÿå†…å­˜
    system_memory = psutil.virtual_memory().total
    available_memory = psutil.virtual_memory().available
    
    logger.info(f"ç³»ç»Ÿæ€»å†…å­˜: {format_memory_size(system_memory)}")
    logger.info(f"å¯ç”¨å†…å­˜: {format_memory_size(available_memory)}")
    
    for result in results:
        model_name = result['model_name']
        logger.info(f"\næ¨¡å‹: {model_name}")
        
        # æ£€æŸ¥æ¯ç§ç­–ç•¥çš„å¯è¡Œæ€§
        for strategy, memory_req in result['strategies'].items():
            if memory_req < available_memory * 0.8:  # ä½¿ç”¨80%çš„å¯ç”¨å†…å­˜
                status = "âœ… å¯è¡Œ"
            elif memory_req < system_memory * 0.8:
                status = "âš ï¸  éœ€è¦å…³é—­å…¶ä»–ç¨‹åº"
            else:
                status = "âŒ å†…å­˜ä¸è¶³"
            
            logger.info(f"  {strategy}: {format_memory_size(memory_req)} - {status}")

def create_memory_optimization_plan():
    """åˆ›å»ºå†…å­˜ä¼˜åŒ–è®¡åˆ’"""
    logger.info(f"\n{'='*60}")
    logger.info("å†…å­˜ä¼˜åŒ–å»ºè®®")
    logger.info(f"{'='*60}")
    
    optimizations = [
        {
            'name': 'æ¢¯åº¦æ£€æŸ¥ç‚¹',
            'description': 'ä½¿ç”¨gradient checkpointingå‡å°‘æ¿€æ´»å†…å­˜',
            'memory_saving': '50-70%',
            'implementation': 'model.gradient_checkpointing_enable()'
        },
        {
            'name': 'æ··åˆç²¾åº¦',
            'description': 'ä½¿ç”¨float16å‡å°‘å†…å­˜ä½¿ç”¨',
            'memory_saving': '50%',
            'implementation': 'torch.cuda.amp.autocast()'
        },
        {
            'name': 'å‚æ•°åˆ†ç‰‡',
            'description': 'å°†æ¨¡å‹å‚æ•°åˆ†ç‰‡åˆ°å¤šä¸ªè®¾å¤‡',
            'memory_saving': 'æŒ‰è®¾å¤‡æ•°é‡çº¿æ€§å‡å°‘',
            'implementation': 'torch.nn.parallel.DistributedDataParallel'
        },
        {
            'name': 'Hessianè¿‘ä¼¼',
            'description': 'ä½¿ç”¨L-BFGSæˆ–æœ‰é™å·®åˆ†è¿‘ä¼¼Hessian',
            'memory_saving': '90%+',
            'implementation': 'scipy.optimize.L-BFGS-B'
        },
        {
            'name': 'æ‰¹å¤„ç†å‡å°‘',
            'description': 'å‡å°‘æ‰¹å¤„ç†å¤§å°',
            'memory_saving': 'çº¿æ€§å‡å°‘',
            'implementation': 'batch_size=1'
        },
        {
            'name': 'åºåˆ—é•¿åº¦é™åˆ¶',
            'description': 'é™åˆ¶è¾“å…¥åºåˆ—é•¿åº¦',
            'memory_saving': 'çº¿æ€§å‡å°‘',
            'implementation': 'max_length=64'
        }
    ]
    
    for opt in optimizations:
        logger.info(f"\n{opt['name']}:")
        logger.info(f"  æè¿°: {opt['description']}")
        logger.info(f"  å†…å­˜èŠ‚çœ: {opt['memory_saving']}")
        logger.info(f"  å®ç°: {opt['implementation']}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ” Hessianå†…å­˜æ¶ˆè€—åˆ†æå·¥å…·")
    logger.info("=" * 60)
    
    # åˆ†æå¤šä¸ªæ¨¡å‹
    results = analyze_multiple_models()
    
    if results:
        # æ¨èç­–ç•¥
        recommend_strategy(results)
        
        # ä¼˜åŒ–å»ºè®®
        create_memory_optimization_plan()
        
        # ä¿å­˜ç»“æœ
        import json
        with open('hessian_memory_analysis.json', 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        logger.info(f"\nâœ… åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° hessian_memory_analysis.json")
    else:
        logger.error("âŒ åˆ†æå¤±è´¥")

if __name__ == "__main__":
    main()
