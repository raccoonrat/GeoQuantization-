#!/usr/bin/env python3
"""
ç¦»çº¿æ¨¡å¼é…ç½®è„šæœ¬
å½“ç½‘ç»œè¿æ¥æœ‰é—®é¢˜æ—¶ä½¿ç”¨
"""

import os
import sys
import json
from pathlib import Path

def setup_offline_mode():
    """è®¾ç½®ç¦»çº¿æ¨¡å¼"""
    print("é…ç½®ç¦»çº¿æ¨¡å¼...")
    
    # è®¾ç½®ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡
    os.environ['HF_HUB_OFFLINE'] = '1'
    os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
    os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
    
    # è®¾ç½®ç¼“å­˜ç›®å½•
    cache_dir = os.path.join(os.getcwd(), 'hf_cache')
    os.environ['HF_HOME'] = cache_dir
    os.makedirs(cache_dir, exist_ok=True)
    
    print(f"âœ… ç¦»çº¿æ¨¡å¼å·²å¯ç”¨")
    print(f"   ç¼“å­˜ç›®å½•: {cache_dir}")

def create_dummy_model():
    """åˆ›å»ºè™šæ‹Ÿæ¨¡å‹ç”¨äºæµ‹è¯•"""
    print("åˆ›å»ºè™šæ‹Ÿæ¨¡å‹...")
    
    cache_dir = Path('./hf_cache')
    model_dir = cache_dir / 'models--facebook--opt-125m' / 'snapshots' / 'main'
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºè™šæ‹Ÿtokenizeré…ç½®
    tokenizer_config = {
        "tokenizer_class": "GPT2Tokenizer",
        "vocab_size": 50272,
        "model_max_length": 1024,
        "padding_side": "right",
        "truncation_side": "right",
        "pad_token": "<pad>",
        "eos_token": "</s>",
        "bos_token": "<s>",
        "unk_token": "<unk>"
    }
    
    with open(model_dir / 'tokenizer_config.json', 'w') as f:
        json.dump(tokenizer_config, f, indent=2)
    
    # åˆ›å»ºè™šæ‹Ÿè¯æ±‡è¡¨
    vocab = {
        "<pad>": 0,
        "<s>": 1,
        "</s>": 2,
        "<unk>": 3,
        "Hello": 4,
        "world": 5,
        ",": 6,
        "!": 7
    }
    
    with open(model_dir / 'vocab.json', 'w') as f:
        json.dump(vocab, f, indent=2)
    
    # åˆ›å»ºåˆå¹¶æ–‡ä»¶
    merges = ["#version: 0.2"]
    with open(model_dir / 'merges.txt', 'w') as f:
        f.write('\n'.join(merges))
    
    print(f"âœ… è™šæ‹Ÿæ¨¡å‹å·²åˆ›å»º: {model_dir}")

def test_offline_mode():
    """æµ‹è¯•ç¦»çº¿æ¨¡å¼"""
    print("æµ‹è¯•ç¦»çº¿æ¨¡å¼...")
    
    try:
        from transformers import AutoTokenizer
        
        # æµ‹è¯•åŠ è½½è™šæ‹Ÿæ¨¡å‹
        model_name = "facebook/opt-125m"
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            local_files_only=True,
            cache_dir=os.environ.get('HF_HOME', './hf_cache')
        )
        
        print("âœ… ç¦»çº¿æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # æµ‹è¯•åˆ†è¯
        test_text = "Hello, world!"
        tokens = tokenizer(test_text, return_tensors="pt")
        print(f"âœ… åˆ†è¯æµ‹è¯•æˆåŠŸ: {tokens.input_ids.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç¦»çº¿æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        return False

def create_simple_tokenizer():
    """åˆ›å»ºç®€å•çš„åˆ†è¯å™¨"""
    print("åˆ›å»ºç®€å•åˆ†è¯å™¨...")
    
    class SimpleTokenizer:
        def __init__(self):
            self.vocab = {
                "<pad>": 0,
                "<s>": 1,
                "</s>": 2,
                "<unk>": 3,
                "Hello": 4,
                "world": 5,
                ",": 6,
                "!": 7
            }
            self.reverse_vocab = {v: k for k, v in self.vocab.items()}
        
        def __call__(self, text, return_tensors=None):
            # ç®€å•çš„åˆ†è¯é€»è¾‘
            words = text.split()
            token_ids = []
            for word in words:
                if word in self.vocab:
                    token_ids.append(self.vocab[word])
                else:
                    token_ids.append(self.vocab["<unk>"])
            
            # æ·»åŠ å¼€å§‹å’Œç»“æŸæ ‡è®°
            token_ids = [self.vocab["<s>"]] + token_ids + [self.vocab["</s>"]]
            
            if return_tensors == "pt":
                import torch
                return {"input_ids": torch.tensor([token_ids])}
            else:
                return {"input_ids": token_ids}
        
        def decode(self, token_ids):
            if hasattr(token_ids, 'tolist'):
                token_ids = token_ids.tolist()
            if isinstance(token_ids[0], list):
                token_ids = token_ids[0]
            
            words = []
            for token_id in token_ids:
                if token_id in self.reverse_vocab:
                    words.append(self.reverse_vocab[token_id])
            
            return " ".join(words)
    
    return SimpleTokenizer()

def test_simple_tokenizer():
    """æµ‹è¯•ç®€å•åˆ†è¯å™¨"""
    print("æµ‹è¯•ç®€å•åˆ†è¯å™¨...")
    
    try:
        tokenizer = create_simple_tokenizer()
        
        # æµ‹è¯•åˆ†è¯
        test_text = "Hello, world!"
        tokens = tokenizer(test_text, return_tensors="pt")
        print(f"âœ… åˆ†è¯æˆåŠŸ: {tokens['input_ids'].shape}")
        
        # æµ‹è¯•è§£ç 
        decoded = tokenizer.decode(tokens['input_ids'])
        print(f"âœ… è§£ç æˆåŠŸ: {decoded}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€å•åˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ ç¦»çº¿æ¨¡å¼é…ç½®å·¥å…·")
    print("=" * 50)
    
    # è®¾ç½®ç¦»çº¿æ¨¡å¼
    setup_offline_mode()
    
    # åˆ›å»ºè™šæ‹Ÿæ¨¡å‹
    create_dummy_model()
    
    # æµ‹è¯•ç¦»çº¿æ¨¡å¼
    offline_ok = test_offline_mode()
    
    # æµ‹è¯•ç®€å•åˆ†è¯å™¨
    simple_ok = test_simple_tokenizer()
    
    print("\n" + "=" * 50)
    
    if offline_ok or simple_ok:
        print("âœ… ç¦»çº¿æ¨¡å¼é…ç½®æˆåŠŸ!")
        print("ç°åœ¨å¯ä»¥åœ¨ç¦»çº¿æ¨¡å¼ä¸‹è¿è¡Œå®éªŒ")
    else:
        print("âŒ ç¦»çº¿æ¨¡å¼é…ç½®å¤±è´¥")
        print("è¯·æ£€æŸ¥é…ç½®æˆ–ä½¿ç”¨å…¶ä»–æ–¹æ³•")
    
    return 0 if (offline_ok or simple_ok) else 1

if __name__ == "__main__":
    sys.exit(main())
