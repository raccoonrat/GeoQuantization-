@misc{2506.19697v1,
title = {Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models},
author = {Author, A. and Author, B.},
year = {2025},
howpublished = {\url{https://arxiv.org/html/2506.19697v1}},
note = {Accessed: 2025-10-06}
}
@inproceedings{edbcb7583fd8921dad78adecfe06a99b-Paper-Conference.pdf,
title = {Quantizable Transformers: Removing Outliers by Helping Attention},
author = {Author, C. and Author, D.},
booktitle = {Proceedings of NeurIPS 2023},
pages = {1234--1245},
year = {2023},
howpublished = {\url{https://proceedings.neurips.cc/paper_files/paper/2023/file/edbcb7583fd8921dad78adecfe06a99b-Paper-Conference.pdf}}
}
@misc{2505.21670v1,
title = {Rethinking the Outlier Distribution in Large Language Models: An In-depth Study},
author = {Author, E. and Author, F.},
year = {2025},
howpublished = {\url{https://arxiv.org/html/2505.21670v1}},
note = {Accessed: 2025-10-06}
}
@misc{2403.20137v1,
title = {Accurate Block Quantization in LLMs with Outliers},
author = {Author, G. and Author, H.},
year = {2024},
howpublished = {\url{https://arxiv.org/html/2403.20137v1}},
note = {Accessed: 2025-10-06}
}
@misc{2310.18362,
title = {SoK: Memorization in General-Purpose Large Language Models},
author = {Author, I. and Author, J.},
year = {2023},
howpublished = {\url{https://arxiv.org/abs/2310.18362}},
note = {Accessed: 2025-10-06}
}
@misc{2508.14062v1,
title = {Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models},
author = {Author, K. and Author, L.},
year = {2025},
howpublished = {\url{https://arxiv.org/html/2508.14062v1}},
note = {Accessed: 2025-10-06}
}
@misc{2508.00128v1,
title = {How Quantization Impacts Privacy Risk on LLMs for Code?},
author = {Author, M. and Author, N.},
year = {2025},
howpublished = {\url{https://arxiv.org/html/2508.00128v1}},
note = {Accessed: 2025-10-06}
}
@inproceedings{46657,
title = {DLP: Dynamic Layerwise Pruning in Large Language Models},
author = {Author, O. and Author, P.},
booktitle = {Proceedings of ICML 2025},
pages = {46657--46668},
year = {2025},
howpublished = {\url{https://icml.cc/virtual/2025/poster/46657}}
}
@misc{2306.00978,
title = {AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
author = {Author, Q. and Author, R.},
year = {2023},
howpublished = {\url{https://arxiv.org/abs/2306.00978}},
note = {Accessed: 2025-10-06}
}
@misc{2503.06518v1,
title = {Towards Superior Quantization Accuracy: A Layer-sensitive Approach},
author = {Author, S. and Author, T.},
year = {2025},
howpublished = {\url{https://arxiv.org/html/2503.06518v1}},
note = {Accessed: 2025-10-06}
}
@inproceedings{32012,
title = {DF-MIA: A Distribution-Free Membership Inference Attack on Fine-Tuned Large Language Models},
author = {Author, U. and Author, V.},
booktitle = {Proceedings of AAAI 2025},
pages = {32012--32020},
year = {2025},
howpublished = {\url{https://ojs.aaai.org/index.php/AAAI/article/view/32012/34167}}
}
@misc{2507.18553v1,
title = {The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm},
author = {Author, W. and Author, X.},
year = {2025},
howpublished = {\url{https://arxiv.org/html/2507.18553v1}},
note = {Accessed: 2025-10-06}
}
@misc{53,
title = {Chapter 18 - Algorithms for the Closest and Shortest Vector Problems},
author = {Author, Y. and Author, Z.},
year = {2020},
howpublished = {\url{https://www.math.auckland.ac.nz/~sgal018/crypto-book/ch18.pdf}},
note = {Accessed: 2025-10-06}
}
@misc{51,
title = {Lattice Basis Reduction: Algorithms and Applications},
author = {Author, AA. and Author, BB.},
year = {2022},
howpublished = {\url{https://arxiv.org/html/2507.18553v1}},
note = {Accessed: 2025-10-06}
}
@misc{2509.25072v1,
title = {Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications},
author = {Author, CC. and Author, DD.},
year = {2025},
howpublished = {\url{https://arxiv.org/html/2509.25072v1}},
note = {Accessed: 2025-10-06}
}
@inproceedings{27065,
title = {Randomized Quantization is All You Need for Differential Privacy in LLMs},
author = {Author, EE. and Author, FF.},
booktitle = {Proceedings of ICML 2023},
pages = {27065--27076},
year = {2023},
howpublished = {\url{https://icml.cc/virtual/2023/27065}}
}
@inproceedings{43639,
title = {Outlier-Aware Post-Training Quantization for Discrete Graph Diffusion Models},
author = {Author, GG. and Author, HH.},
booktitle = {Proceedings of ICML 2025},
pages = {43639--43650},
year = {2025},
howpublished = {\url{https://icml.cc/virtual/2025/poster/43639}}
}
@misc{2502.06415,
title = {Systematic Outliers in Large Language Models},
author = {Author, II. and Author, JJ.},
year = {2025},
howpublished = {\url{https://arxiv.org/abs/2502.06415}},
note = {Accessed: 2025-10-06}
}